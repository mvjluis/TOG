{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASTER R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction\n",
    "# create random image\n",
    "image = np.zeros((1,3,800,800))\n",
    "#change type to float\n",
    "image = image.astype(np.float32)\n",
    "# convert to tensor\n",
    "image = tf.convert_to_tensor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using just tensorflow to create generic data of ...\n",
    "# image\n",
    "image = tf.zeros([1,800,800,3], tf.float32)\n",
    "# bbox\n",
    "bbox = tf.constant([[20, 30, 400, 500], [300, 400, 500, 600]])\n",
    "# labels for each bbox\n",
    "labels = tf.constant([6,8]) \n",
    "sub_sample = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VGG16 network is used as a feature extraction module here, This acts as a backbone for both the RPN network and Fast_R-CNN network. We need to make a few changes to the VGG network inorder to make this work. Since the input of the network is 800, the output of the feature extraction module should have a feature map size of (800//16). So we need to check where the VGG16 module is achieving this feature map size and trim the network till der. This can be done in the following way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch implementation:\n",
    "https://medium.com/@fractaldle/guide-to-build-faster-rcnn-in-pytorch-95b10c273439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 ) VGG16  import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default input size for this model is 224x224.\n",
    "vgg = VGG16(weights='imagenet', include_top=False, input_tensor=layers.Input(shape=[800, 800, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x1f394c81d88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f394c8ff08>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f394c9bc08>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1f394cabb08>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f3dce0b448>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f3dce21c48>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1f3dce26fc8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f3dce36d48>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f3dce12b88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f3dce3e808>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1f3dce4c048>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f3dce53d88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f3dce5fac8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f3dce64548>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1f3dce657c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f3dce6fb08>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f3dce7fa48>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f3dce834c8>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1f3dce85748>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the layers\n",
    "vgg.layers\n",
    "# or uncomment the next line\n",
    "#vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vgg feature layers:  19\n",
      "brake\n",
      "shape of ouput:  (1, 50, 50, 512)\n",
      "number of layers needed:  18\n"
     ]
    }
   ],
   "source": [
    "#how to pass a input test tensor through the layers without compile the model?\n",
    "im = image\n",
    "fe_layers = vgg.layers\n",
    "l_c = 0\n",
    "req_layers = []\n",
    "print(\"size of vgg feature layers: \", len(fe_layers))\n",
    "# Use CPU to test the convolutions\n",
    "with tf.device('CPU:0'):\n",
    "    for i in fe_layers:\n",
    "        im = i(im)\n",
    "        if im.shape[1] < 800//16:\n",
    "            print('brake')\n",
    "            break\n",
    "        req_layers.append(i)\n",
    "        l_c += 1\n",
    "        out_channels = im.shape\n",
    "print('shape of ouput: ', out_channels)\n",
    "print('number of layers needed: ', l_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x000001F394C81D88>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F394C8FF08>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F394C9BC08>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001F394CABB08>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F3DCE0B448>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F3DCE21C48>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001F3DCE26FC8>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F3DCE36D48>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F3DCE12B88>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F3DCE3E808>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001F3DCE4C048>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F3DCE53D88>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F3DCE5FAC8>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F3DCE64548>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001F3DCE657C8>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F3DCE6FB08>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F3DCE7FA48>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F3DCE834C8>]\n"
     ]
    }
   ],
   "source": [
    "# print the required layers\n",
    "print(req_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the bakcbone for fast r-cnn\n",
    "input_fe = layers.Input(shape=[800, 800, 3])\n",
    "fe_extractor = req_layers[1](input_fe)\n",
    "for l in range(2,len(req_layers)):\n",
    "    fe_extractor = req_layers[l](fe_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of output:  (1, 50, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "# Test feature extractor model output\n",
    "fe_model = Model(inputs=input_fe, outputs=fe_extractor)\n",
    "out_map = image\n",
    "with tf.device('CPU:0'):\n",
    "    out_map = fe_model(out_map)\n",
    "print(\"shape of output: \",out_map.shape)\n",
    "del(fe_model)\n",
    "\n",
    "# Input = input_fe,\n",
    "# Output of extractor feature = fe_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Anchor boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use anchor_scales of 8, 16, 32, ratio of 0.5, 1, 2 and sub sampling of 16 (Since we have pooled our image from 800 px to 50px). Now every pixel in the output feature map maps to corresponding 16 * 16 pixels in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each pixel location on the feature map, We need to generate 9 anchor boxes (number of anchor_scales and number of ratios) and each anchor box will have ‘y1’, ‘x1’, ‘y2’, ‘x2’. So at each location anchor will have a shape of (9, 4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# As an example will be created 9 anchor boxes space for 'y1', 'x1', 'y2', 'x2'\n",
    "ratios = [0.5, 1, 2]\n",
    "# 0.5 : ratio 2:1; 1 : ratio 1:1; 2 : ratio 1:2;\n",
    "anchor_scales = [8, 16, 32]\n",
    "anchor_base = np.zeros((len(ratios)*len(anchor_scales), 4), dtype=np.float32)\n",
    "\n",
    "print(anchor_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center of the sample anchor boxes: yc = 8.0, xc = 8.0\n",
      "[[ -37.254833  -82.50967    53.254833   98.50967 ]\n",
      " [ -82.50967  -173.01933    98.50967   189.01933 ]\n",
      " [-173.01933  -354.03867   189.01933   370.03867 ]\n",
      " [ -56.        -56.         72.         72.      ]\n",
      " [-120.       -120.        136.        136.      ]\n",
      " [-248.       -248.        264.        264.      ]\n",
      " [ -82.50967   -37.254833   98.50967    53.254833]\n",
      " [-173.01933   -82.50967   189.01933    98.50967 ]\n",
      " [-354.03867  -173.01933   370.03867   189.01933 ]]\n"
     ]
    }
   ],
   "source": [
    "# example to fill the anchor x and y coordinates values for the 9 anchor boxes of the respective size and ratio\n",
    "sub_sample = 16\n",
    "ctr_y = sub_sample / 2.\n",
    "ctr_x = sub_sample / 2.\n",
    "\n",
    "print('center of the sample anchor boxes: yc = {}, xc = {}'.format(ctr_y, ctr_x))\n",
    "for i in range(len(ratios)):\n",
    "    for j in range(len(anchor_scales)):\n",
    "        h = sub_sample * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "        w = sub_sample * anchor_scales[j] * np.sqrt(1./ ratios[i])\n",
    "        #print(\"height: {}, width: {}\".format(h, w))\n",
    "        \n",
    "        index = i * len(anchor_scales) + j\n",
    "        \n",
    "        anchor_base[index, 0] = ctr_y - h/ 2.\n",
    "        anchor_base[index, 1] = ctr_x - w/ 2.\n",
    "        anchor_base[index, 2] = ctr_y + h/ 2.\n",
    "        anchor_base[index, 3] = ctr_x + w/ 2.\n",
    "\n",
    "print(anchor_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the anchor locations at the first feature map pixel, we have to now generate these anchors at all the locations of feature map. \n",
    "\n",
    "In the later section we will label them with -1 and remove them when calculating the loss the functions and generating proposals for anchor boxes.\n",
    "\n",
    "Also Since we got 9 anchors at each location and there 50 * 50 such locations inside an image, We will get 17500 ((50 * 50 * 9) - (number of invalid anchor boxes)) anchors in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate anchors for all the feature map (50 x 50 x 512) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 16,  32,  48,  64,  80,  96, 112, 128, 144, 160, 176, 192, 208,\n",
       "       224, 240, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416,\n",
       "       432, 448, 464, 480, 496, 512, 528, 544, 560, 576, 592, 608, 624,\n",
       "       640, 656, 672, 688, 704, 720, 736, 752, 768, 784, 800])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_size = (800//16)\n",
    "ctr_x = np.arange(16, (fe_size+1) * 16, 16)\n",
    "ctr_y = np.arange(16, (fe_size+1) * 16, 16)\n",
    "\n",
    "ctr_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the center coordinates for every anchor box\n",
    "index = 0\n",
    "ctr = []\n",
    "#print(ctr[0][1])\n",
    "for x in range(len(ctr_x)):\n",
    "    for y in range(len(ctr_y)):\n",
    "        #print(index)\n",
    "        ctr.append([ctr_y[y] - 8 , ctr_x[x] - 8])\n",
    "        index += 1\n",
    "ctr = np.array(ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x2880 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the center of the anchor boxes\n",
    "plt.figure(figsize=(40,40))\n",
    "plt.scatter(ctr[:,1],ctr[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will be the (x, y) value at each location as shown in the image above. Together we have 2500 anchor centers. Now at each center we need to generate the anchor boxes. This can be done using the code we have used for generating anchor at one location, adding an extract for loop for supplying centers of each anchor will do. Lets see how this is done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 4)\n"
     ]
    }
   ],
   "source": [
    "# Create the 9 anchor box for every center\n",
    "anchors = np.zeros(((fe_size*fe_size*9), 4))\n",
    "index = 0\n",
    "for c in ctr:\n",
    "    ctr_y, ctr_x = c\n",
    "    for i in range(len(ratios)):\n",
    "        for j in range(len(anchor_scales)):\n",
    "            h = sub_sample * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "            w = sub_sample * anchor_scales[j] * np.sqrt(1./ ratios[i])\n",
    "            \n",
    "            anchors[index, 0] = ctr_y - h/ 2.\n",
    "            anchors[index, 1] = ctr_x - w/ 2.\n",
    "            anchors[index, 2] = ctr_y + h/ 2.\n",
    "            anchors[index, 3] = ctr_x + w/ 2.\n",
    "            index += 1\n",
    "            \n",
    "print(anchors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are better way of generating anchor boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940,)\n"
     ]
    }
   ],
   "source": [
    "# Get just the valid indexes of the anchor boxes\n",
    "index_inside = np.where((\n",
    "    (anchors[:, 0] >= 0) &\n",
    "    (anchors[:, 1] >= 0) &\n",
    "    (anchors[:, 2] <= 800) &\n",
    "    (anchors[:, 3] <= 800)\n",
    "    ))[0]\n",
    "print(index_inside.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label shape: (8940,)\n",
      "valid anchor boxes: (8940, 4)\n"
     ]
    }
   ],
   "source": [
    "# create an empty array with inside_index shape and fill with -1\n",
    "label = np.empty((len(index_inside), ), dtype=np.int32)\n",
    "label.fill(-1)\n",
    "print(\"label shape: {}\".format(label.shape))\n",
    "\n",
    "# array with valid anchors\n",
    "valid_anchor_boxes = anchors[index_inside]\n",
    "print(\"valid anchor boxes: {}\".format(valid_anchor_boxes.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOU calculation for every anchor box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of ground truth bbox and labels\n",
    "bbox = np.asarray([[20, 30, 400, 500],[300, 400, 500, 600]], dtype=np.float32)\n",
    "# labels of the bbox\n",
    "labels = np.asarray([6,8], dtype=np.int8)\n",
    "# 0 represent background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$IOU = \\frac{A \\cap B}{A \\cup B}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A = Area of ground truth;\n",
    "\n",
    "B = Area of anchor box;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$IOU = \\frac{Area of intersection}{Area of ground truth + Area of anchor - Area of intersection}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ious shape: (8940, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create array of iou ( intersection over union) of every anchor box\n",
    "ious = np.empty((len(valid_anchor_boxes), len(bbox)), dtype=np.float32)\n",
    "ious.fill(0)\n",
    "for num1, i in enumerate(valid_anchor_boxes):\n",
    "    #num1 = enumeration of anchor\n",
    "    #i = anchor box [y1, x1, y2, x2]\n",
    "    ya1, xa1, ya2, xa2 = i\n",
    "    anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
    "    for num2, j in enumerate(bbox):\n",
    "        #num2 = enumeration of bbox with ground truth\n",
    "        #j = bbox with ground truth\n",
    "        yb1, xb1, yb2, xb2 = j\n",
    "        box_area = (yb2 - yb1) * (xb2 - xb1)\n",
    "        \n",
    "        inter_x1 = max([xb1, xa1])\n",
    "        inter_y1 = max([yb1, ya1])\n",
    "        inter_x2 = min([xb2, xa2])\n",
    "        inter_y2 = min([yb2, ya2])\n",
    "        \n",
    "        if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
    "            iter_area = (inter_y2 - inter_y1) * (inter_x2 - inter_x1)\n",
    "            iou = iter_area / (anchor_area + box_area - iter_area)\n",
    "        else:\n",
    "            iou = 0.\n",
    "        ious[num1, num2] = iou\n",
    "\n",
    "print(\"ious shape: {}\".format(ious.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Using numpy arrays, these calculations can be done much more efficiently and with less verbose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since we have generated all the anchor boxes, we need to look at the objects inside the image and assign them to the specific anchor boxes which contain them. Faster_R-CNN has some guidelines to assign labels to the anchor boxes\n",
    "We assign a positive label to two kind of anchors \n",
    "\n",
    "a) The anchor/anchors with the highest Intersection-over-Union(IoU) overlap with a ground-truth-box or \n",
    "\n",
    "b) An anchor that has an IoU overlap higher than 0.7 with ground-truth box.\n",
    "Note that single ground-truth object may assign positive labels to multiple anchors.\n",
    "\n",
    "c) We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. \n",
    "\n",
    "d) Anchors that are neither positive nor negitive do not contribute to the training objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2262 5620]\n",
      "[0.68130493 0.61035156]\n"
     ]
    }
   ],
   "source": [
    "# a) get the index of the greatest IOU for each ground truth bbox\n",
    "# indexes\n",
    "gt_argmax_ious = ious.argmax(axis=0)\n",
    "print(gt_argmax_ious)\n",
    "\n",
    "# IOU value\n",
    "gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
    "print(gt_max_ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940,)\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0.06811669 0.07083762 0.07083762 ... 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# b) get the greater value between the IOUs of every valid anchor box\n",
    "\n",
    "# get the localization of the greater value of IOU of every anchor\n",
    "argmax_ious = ious.argmax(axis=1)\n",
    "print(argmax_ious.shape)\n",
    "print(argmax_ious)\n",
    "\n",
    "# get the value of the greater value of IOU of every valid anchor box\n",
    "max_ious = ious[np.arange(len(index_inside)), argmax_ious]\n",
    "print(max_ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2262 2508 5620 5628 5636 5644 5866 5874 5882 5890 6112 6120 6128 6136\n",
      " 6358 6366 6374 6382]\n"
     ]
    }
   ],
   "source": [
    "# find the anchor boxes which have this max_ious(gt_max_ious)\n",
    "\n",
    "gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
    "print(gt_argmax_ious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have three arrays\n",
    "\n",
    "argmax_ious — Tells which ground truth object has max iou with each anchor.\n",
    "\n",
    "max_ious — Tells the max_iou with ground truth object with each anchor.\n",
    "\n",
    "gt_argmax_ious — Tells the anchors with the highest Intersection-over-Union (IoU) overlap with a ground-truth box.\n",
    "\n",
    "Using argmax_ious and max_ious we can assign labels and locations to anchor boxes which satisify [b] and [c]. Using gt_argmax_ious we can assign labels and locations to anchor boxes which satisify [a]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asign Label to anchor boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_iou_threshold = 0.7\n",
    "neg_iou_threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label array initialized with \"-1\" value \n",
    "label\n",
    "# -1 : No used for training\n",
    "# 0 : No object in the anchor; it is applied if the max_ious[i] < neg_iou_threshold\n",
    "# 1 : Object in the anchor; it is applied if the max_ious[i] >= pos_iou_threshold ...\n",
    "# or if the anchor is one of the gt_argmax_ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no object in anchor\n",
    "label[max_ious < neg_iou_threshold] = 0\n",
    "# object in anchor\n",
    "label[gt_argmax_ious] = 1\n",
    "label[max_ious >= pos_iou_threshold] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of the training anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training RPN The Faster_R-CNN paper phrases as follows Each mini-batch arises from a single image that contains many positive and negitive example anchors, but this will bias towards negitive samples as they are dominate. Instead, we randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of up to 1:1. If there are fewer than 128 positive samples in an image, we pad the mini-batch with negitive ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since it is needed 256 anchors to compute the loss function with equal negative and positive samples\n",
    "# it is defined the next:\n",
    "pos_ratio = 0.5\n",
    "n_sample = 256\n",
    "# Total of positive samples\n",
    "n_pos = pos_ratio * n_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive samples:\n",
    "\n",
    "pos_index = np.where(label == 1)[0]\n",
    " # if there are more than n_pos positive samples, some of them are disabled the get just n_pos samples\n",
    "if len(pos_index) > n_pos:\n",
    "    disable_index = np.random.choice(pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
    "    label[disable_index] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative samples:\n",
    "\n",
    "# Check this line after: n_neg = n_sample * np.sum(label == 1)\n",
    "n_neg = n_sample - np.sum(label == 1)\n",
    "neg_index = np.where(label == 0)[0]\n",
    "\n",
    "if len(neg_index) > n_neg:\n",
    "    disable_index = np.random.choice(neg_index, size=(len(neg_index) - n_neg), replace=False)\n",
    "    label[disable_index] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive samples: 18\n",
      "negative samples: 238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of samples:\n",
    "\n",
    "print(\"positive samples: {}\".format(np.sum(label == 1)))\n",
    "print(\"negative samples: {}\".format(np.sum(label == 0)))\n",
    "(np.sum(label == 1) + np.sum(label == 0)) == n_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asigning locations to anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets assign the locations to each anchor box with the ground truth object which has maximum iou. Note, we will assign anchor locs to all the valid anchor boxes irrespective of its label, later when we are calculating the losses, we can remove them with simple filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know which ground truth object has high iou with each anchor box, Now we need to find the locations of ground truth with respect to the anchor box location. Faster_R-CNN uses the following parametrizion for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$t_{x} = (x - x_{a})/w_{a}$$\n",
    "$$t_{y} = (y - y_{a})/h_{a}$$\n",
    "$$t_{w} = log(w/ w_a)$$\n",
    "$$t_{h} = log(h/ h_a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x, y , w, h are the groud truth box center co-ordinates, width and height. x_a, y_a, h_a and w_a and anchor boxes center cooridinates, width and height.\n",
    "\n",
    "For each anchor box, find the groundtruth object which has max_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 20.  30. 400. 500.]\n",
      " [ 20.  30. 400. 500.]\n",
      " [ 20.  30. 400. 500.]\n",
      " ...\n",
      " [ 20.  30. 400. 500.]\n",
      " [ 20.  30. 400. 500.]\n",
      " [ 20.  30. 400. 500.]]\n"
     ]
    }
   ],
   "source": [
    "# Create an array choosing the ground truth box for which the anchor box vs ground truth (1 or 2 in this example)IOU is greater\n",
    "max_iou_bbox = bbox[argmax_ious]\n",
    "print(max_iou_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 20.,  30., 400., 500.],\n",
       "       [ 20.,  30., 400., 500.],\n",
       "       [ 20.,  30., 400., 500.],\n",
       "       ...,\n",
       "       [ 20.,  30., 400., 500.],\n",
       "       [ 20.,  30., 400., 500.],\n",
       "       [ 20.,  30., 400., 500.]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_iou_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inorder to find $t_{x}$, $t_{y}$, $t_{w}$, $t_{h}$, we need to convert the y1, x1, y2, x2 format of valid anchor boxes and associated ground truth boxes with max iou to ctr_y, ctr_x , h, w format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANCHOR BOXES\n",
    "# height = y2 - y1 = ha\n",
    "height = valid_anchor_boxes[:, 2] - valid_anchor_boxes[:, 0]\n",
    "# width = x2 - x1 = wa\n",
    "width = valid_anchor_boxes[:, 3] - valid_anchor_boxes[:, 1]\n",
    "# Center of the anchor box\n",
    "# ya\n",
    "ctr_y = valid_anchor_boxes[:, 0] + (0.5 * height)\n",
    "# xa\n",
    "ctr_x = valid_anchor_boxes[:, 1] + (0.5 * width)\n",
    "\n",
    "# GROUND TRUTH\n",
    "# h\n",
    "base_height = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]\n",
    "# w\n",
    "base_width = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]\n",
    "# y\n",
    "base_ctr_y = max_iou_bbox[:, 0] + (0.5 * base_height)\n",
    "# x\n",
    "base_ctr_x = max_iou_bbox[:, 1] + (0.5 * base_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above formulas to find the loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5855728   2.30914558  0.7415674   1.64727602]\n",
      " [ 0.49718446  2.30914558  0.7415674   1.64727602]\n",
      " [ 0.40879611  2.30914558  0.7415674   1.64727602]\n",
      " ...\n",
      " [-2.50801936 -5.29225232  0.7415674   1.64727602]\n",
      " [-2.59640771 -5.29225232  0.7415674   1.64727602]\n",
      " [-2.68479606 -5.29225232  0.7415674   1.64727602]]\n"
     ]
    }
   ],
   "source": [
    "# avoiding have values equal to 0\n",
    "eps = np.finfo(height.dtype).eps\n",
    "height = np.maximum(height, eps)\n",
    "width = np.maximum(width, eps)\n",
    "\n",
    "# Calulate the relative localization to the ground truth\n",
    "# y-ya/ha\n",
    "dy = (base_ctr_y - ctr_y) / height\n",
    "# x-xa/wa\n",
    "dx = (base_ctr_x - ctr_x) / width\n",
    "# log(h/ha)\n",
    "dh = np.log(base_height / height)\n",
    "# Log(w/wa)\n",
    "dw = np.log(base_width / width)\n",
    "\n",
    "# pack the relative loc\n",
    "anchor_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "print(anchor_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got anchor_locs and label associated with each and every valid anchor boxes.\n",
    "\n",
    "Lets map them to the original anchors using the inside_index variable. Fill the unvalid anchor boxes labels with -1 (ignore) and locations with 0.\n",
    "\n",
    "Final labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final anchor labels (-1: not use, 0: no object, 1: object) for every anchor box even the not valid ones\n",
    "anchor_labels = np.empty((len(anchors),), dtype=label.dtype)\n",
    "anchor_labels.fill(-1)\n",
    "anchor_labels[index_inside] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the locations for every anchor box even the not valid ones\n",
    "anchor_locations = np.empty((len(anchors), ) + anchors.shape[1:], dtype=anchor_locs.dtype)\n",
    "anchor_locations.fill(0)\n",
    "anchor_locations[index_inside, :] = anchor_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the anchor box labels: (22500,)\n",
      "shape of the the anchor box locations: (22500, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of the anchor box labels: {}\".format(anchor_labels.shape))\n",
    "print(\"shape of the the anchor box locations: {}\".format(anchor_locations.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final two matrices are: (Remember that just 256 samples are going to be used for training the model)\n",
    " \n",
    "anchor_locations [N, 4] — [22500, 4]; which contains the ralative location to the ground truth for which the IOU is bigger\n",
    "\n",
    "anchor_labels [N,] — [22500]; which contains if it is a object or not or if not apply\n",
    "\n",
    "These are used as targets to the RPN network. We will see how this RPN network is designed in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) REGION PROPOSAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as JupImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have discussed earlier, Prior to this work, region proposals for a network were generated using selective search, CPMC, MCG, Edgeboxes etc. Faster_R-CNN is the first work to demonstrate generating region proposals using deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](RPN.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network contains a convolution module, on top of which there will be one regression layer, which predicts the location of the box inside the anchor\n",
    "\n",
    "To generate region proposals, we slide a small network over the convolutional feature map output that we obtained in the feature extraction module. This small network takes as input an n x n spatial window of the input convolutional feature map. Each sliding window is mapped to a lower-dimensional feature [512 features]. This feature is fed into two sibling fully connected layers\n",
    "\n",
    "A box regrression layer\n",
    "\n",
    "A box classification layer\n",
    "\n",
    "we use n=3, as noted in Faster_R-CNN paper. We can implement this Architecture using n x n convolutional layer followed by two sibiling 1 x 1 convolutional layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of the ouput in this case for the VGG feature map is 50x50x512\n",
    "mid_channels = 512\n",
    "in_channels = 512\n",
    "n_anchor = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The paper tells that they initialized these layers with zero mean and 0.01 standard deviation for weights and zeros for base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the RPN Network\n",
    "'''\n",
    " pytorch: conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
    " torch.nn.Conv2d(in_channels: int, out_channels: int, \n",
    "                kernel_size: Union[T, Tuple[T, T]], stride: Union[T, Tuple[T, T]] = 1,\n",
    "                padding: Union[T, Tuple[T, T]] = 0, dilation: Union[T, Tuple[T, T]] = 1,\n",
    "                groups: int = 1, bias: bool = True, padding_mode: str = 'zeros')\n",
    "tf.keras.layers.Conv2D(\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    strides=(1, 1),\n",
    "    padding=\"valid\",\n",
    "    data_format=None,\n",
    "    dilation_rate=(1, 1),\n",
    "    groups=1,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs\n",
    ")\n",
    "'''\n",
    "# 1) Intermediate Layer to reduce the dimentionallity\n",
    "conv1 = layers.Conv2D(filters=1, kernel_size=3, strides=(1, 1), padding='same', activation='relu')\n",
    "\n",
    "# 2) Regression layer are with the activation function which output is 0 to 1 remembering the ouput is \n",
    "# normalized\n",
    "reg_layer = layers.Conv2D(filters=(n_anchor * 4), kernel_size=1, strides=(1, 1), activation='sigmoid', \n",
    "                          kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "                         bias_initializer=initializers.Zeros())\n",
    "\n",
    "# 3) Objectness classification layer use softmax layer to describe if there is a proposal for object or not\n",
    "cls_layer = layers.Conv2D(filters=(n_anchor * 2), kernel_size=1, strides=(1, 1), activation='softmax',\n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "                         bias_initializer=initializers.Zeros())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the outputs we got in the feature extraction state should be sent to this network to predict locations of objects with repect to the anchor and the objectness score assoiciated with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 50, 18) (1, 50, 50, 36)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('CPU:0'):\n",
    "    x = conv1(out_map)\n",
    "    pred_anchor_locs = reg_layer(x)\n",
    "    pred_cls_scores = cls_layer(x)\n",
    "    \n",
    "print(pred_cls_scores.shape, pred_anchor_locs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is needed format the outputs pred_anchor_locs and pred_cls_scores to match with the shape of the anchor boxes format:\n",
    "\n",
    "    Anchor boxes locations shape: [1, 22500, 4]\n",
    "    Anchor boxes objectness shape: [1, 22500, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the output of the regresion layer in RPN: (1, 22500, 4)\n",
      "shape of the objectness score layer in RPN: (1, 22500)\n",
      "shape of the output of the classificarion layer in RPN: (1, 22500, 2)\n"
     ]
    }
   ],
   "source": [
    "# Pytorch:\n",
    "# pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\n",
    "# pred_anchor_locs = pred_anchor_locs.(change the view of position).(create a copy of the tensor).(return a new tensor if the shapes change)\n",
    "# in \".view(1, -1, 4)\" the number -1, tell the function to infer that dimension from the others given.\n",
    "\n",
    "pred_anchor_locs = tf.reshape(pred_anchor_locs, [1, -1, 4])\n",
    "print(\"shape of the output of the regresion layer in RPN: {}\".format(pred_anchor_locs.shape))\n",
    "\n",
    "objectness_score = tf.reshape(tf.reshape(pred_cls_scores, [1, 50, 50, 9, 2])[:, :, :, :, 1], [1, -1])\n",
    "print(\"shape of the objectness score layer in RPN: {}\".format(objectness_score.shape))\n",
    "\n",
    "pred_cls_scores = tf.reshape(pred_cls_scores, [1, -1, 2])\n",
    "print(\"shape of the output of the classificarion layer in RPN: {}\".format(pred_cls_scores.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred_cls_scores and pred_anchor_locs are the output the RPN network and the losses to updates the weights.\n",
    "\n",
    "pred_cls_scores and objectness_scores are used as inputs to the __proposal layer__, which generate a set of proposal which are further used by RoI network. We will see this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate proposals to feed the Fast R-CNN network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proposal function will take the following parameters\n",
    "Weather training_mode or testing mode\n",
    "\n",
    "nms_thresh\n",
    "\n",
    "n_train_pre_nms — number of bboxes before nms during training\n",
    "\n",
    "n_train_post_nms — number of bboxes after nms during training\n",
    "\n",
    "n_test_pre_nms — number of bboxes before nms during testing\n",
    "\n",
    "n_test_post_nms — number of bboxes after nms during testing\n",
    "\n",
    "min_size — minimum height of the object required to create a proposal.\n",
    "\n",
    "The Faster R_CNN says, RPN proposals highly overlap with each other. To reduced redundancy, we adopt non-maximum supression (NMS) on the proposal regions based on their cls scores. We fix the IoU threshold for NMS at 0.7, which leaves us about 2000 proposal regions per image. After an ablation study, the authors show that NMS does not harm the ultimate detection accuracy, but substantially reduces the number of proposals. After NMS, we use the top-N ranked proposal regions for detection. In the following we training Fast R-CNN using 2000 RPN proposals. During testing they evaluate only 300 proposals, they have tested this with various numbers and obtained this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_thresh = 0.7\n",
    "n_train_pre_nms = 12000\n",
    "n_train_post_nms = 2000\n",
    "n_test_pre_nms = 6000\n",
    "n_test_post_nms = 300\n",
    "min_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do the following things to generate region of interest proposals to the network.\n",
    "\n",
    "convert the loc predictions from the rpn network to bbox [y1, x1, y2, x2] format.\n",
    "\n",
    "clip the predicted boxes to the image\n",
    "\n",
    "Remove predicted boxes with either height or width < threshold (min_size).\n",
    "\n",
    "Sort all (proposal, score) pairs by score from highest to lowest.\n",
    "\n",
    "Take top pre_nms_topN (e.g. 12000 while training and 300 while testing).\n",
    "\n",
    "Apply nms threshold > 0.7\n",
    "\n",
    "Take top pos_nms_topN (e.g. 2000 while training and 300 while testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Convert the loc predictions from the RPN network to bbox [y1, x1, y2, x2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation decodes predictions by un-parameterizing them and offseting to image. the formulas are as follows:\n",
    "\n",
    "\n",
    "$$ x = w_{a} * ctr_x_{p} + ctr_x_{a} $$\n",
    "$$ y = (h_{a} * ctr_x_{p}) + ctr_x_{a} $$\n",
    "$$ h = e^{h_{p}} * h_{a} $$\n",
    "$$ w = e^{w_{p}} * w_{a} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert anchors format from y1, x1, y2, x2 to ctr_x, ctr_y, h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y2 - y1\n",
    "anc_height = anchors[:, 2] - anchors[:, 0]\n",
    "# x2 - x1\n",
    "anc_width = anchors[:, 3] - anchors[:, 1]\n",
    "# y1 + 0.5*height_a\n",
    "anc_ctr_y = anchors[:, 0] + (0.5 * anc_height)\n",
    "# x1 + 0.5*width_1\n",
    "anc_ctr_x = anchors[:, 1] + (0.5 * anc_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert predictions locs using above formulas. before that convert the pred_anchor_locs and objectness_score to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 1)\n",
      "(22500, 1)\n",
      "(22500, 1)\n",
      "(22500, 1)\n",
      "(22500, 4)\n",
      "(22500,)\n"
     ]
    }
   ],
   "source": [
    "# change tensor to numpy\n",
    "pred_anchor_locs_numpy = pred_anchor_locs[0].numpy()\n",
    "objectness_score_numpy = objectness_score[0].numpy()\n",
    "\n",
    "dy = pred_anchor_locs_numpy[:, 0::4] # choose all the dy\n",
    "dx = pred_anchor_locs_numpy[:, 1::4] # choose all the dx\n",
    "dh = pred_anchor_locs_numpy[:, 2::4] # choose all the dh\n",
    "dw = pred_anchor_locs_numpy[:, 3::4] # choose all the dw\n",
    "\n",
    "print(dx.shape)\n",
    "print(dy.shape)\n",
    "print(dh.shape)\n",
    "print(dw.shape)\n",
    "print(pred_anchor_locs_numpy.shape)\n",
    "print(objectness_score_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the resized data of the anchor localization\n",
    "ctr_y = dy * anc_height[:, np.newaxis] + anc_ctr_y[:, np.newaxis]\n",
    "ctr_x = dx * anc_width[:, np.newaxis] + anc_ctr_x[:, np.newaxis]\n",
    "h = np.exp(dh) * anc_height[:, np.newaxis]\n",
    "w = np.exp(dw) * anc_width[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert [ctr_x, ctr_y, h, w] to [y1, x1, y2, x2] format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert resized data parameters to the anchor box format [y1, x1, y2, x2]\n",
    "roi = np.zeros(pred_anchor_locs_numpy.shape, dtype=pred_anchor_locs_numpy.dtype)\n",
    "\n",
    "roi[:, 0::4] = ctr_y - (0.5 * h)\n",
    "roi[:, 1::4] = ctr_x - (0.5 * w)\n",
    "roi[:, 2::4] = ctr_y + (0.5 * h)\n",
    "roi[:, 3::4] = ctr_x + (0.5 * w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -21.508091,  -50.559566,  127.78874 ,  248.0284  ],\n",
       "       [ -50.86635 , -110.01628 ,  247.50494 ,  487.03027 ],\n",
       "       [-110.09823 , -226.92953 ,  487.39816 ,  967.55145 ],\n",
       "       ...,\n",
       "       [ 733.4401  ,  762.69434 , 1031.8711  ,  911.8327  ],\n",
       "       [ 675.28143 ,  733.57794 , 1271.4403  , 1031.3539  ],\n",
       "       [ 558.8442  ,  674.811   , 1750.7766  , 1271.6227  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clip the predicted boxes to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (800, 800)\n",
    "# slice(start, end, step), A slice object is used to specify how to slice a sequence.\n",
    "# np.clip(a, a_min, a_max, out=None) \n",
    "# restricting the anchor coordinates to 0-800 (size of the images)\n",
    "\n",
    "# For Y coordinates\n",
    "roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
    "\n",
    "# For X coordinates\n",
    "roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.     ,   0.     , 127.78874, 248.0284 ],\n",
       "       [  0.     ,   0.     , 247.50494, 487.03027],\n",
       "       [  0.     ,   0.     , 487.39816, 800.     ],\n",
       "       ...,\n",
       "       [733.4401 , 762.69434, 800.     , 800.     ],\n",
       "       [675.28143, 733.57794, 800.     , 800.     ],\n",
       "       [558.8442 , 674.811  , 800.     , 800.     ]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove predicted boxes with either height or width < threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500,)\n"
     ]
    }
   ],
   "source": [
    "# Get the height and width of every anchor box\n",
    "hs = roi[:, 2] - roi[:, 0]\n",
    "ws = roi[:, 3] - roi[:, 1]\n",
    "\n",
    "# Select just those ones that the height and width are >= min_size\n",
    "keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
    "roi = roi[keep, :]\n",
    "\n",
    "# Select the scores of those ones that the height and width are >= min_size\n",
    "score = objectness_score_numpy[keep]\n",
    "\n",
    "print(score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort all (proposal, score) pairs by score from highest to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[442  28  10 ...  11  29 443]\n",
      "[[732.93536   0.      800.      486.45236]\n",
      " [  0.        0.      295.26608 486.57367]\n",
      " [  0.        0.      263.27228 486.5855 ]\n",
      " ...\n",
      " [  0.        0.      503.326   800.     ]\n",
      " [  0.        0.      535.3241  800.     ]\n",
      " [673.0241    0.      800.      800.     ]]\n",
      "[0.05654256 0.05642415 0.05641259 ... 0.05505416 0.05504745 0.05497881]\n"
     ]
    }
   ],
   "source": [
    "order = score.ravel().argsort()[::-1]\n",
    "# order = score.(ravel: return a contiguous flattened array or remove dimenstionallity).\n",
    "#            (argsort: Returns the indices that would sort an array)[Revert the order to get major to minor]\n",
    "print(order)\n",
    "print(roi[order])\n",
    "print(score[order])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take top pre_nms_topN (e.g. 12000 while training and 300 while testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 4)\n",
      "[[732.93536   0.      800.      486.45236]\n",
      " [  0.        0.      295.26608 486.57367]\n",
      " [  0.        0.      263.27228 486.5855 ]\n",
      " ...\n",
      " [  0.      125.13783 487.46976 800.     ]\n",
      " [  0.      318.48184 177.51816 529.5182 ]\n",
      " [  0.      276.96368 347.03632 699.0363 ]]\n"
     ]
    }
   ],
   "source": [
    "# Choose the first top 12000 indexes of the score \n",
    "order = order[:n_train_pre_nms]\n",
    "\n",
    "# Get the first top 12000 anchors \n",
    "roi = roi[order, :]\n",
    "\n",
    "\n",
    "print(roi.shape)\n",
    "print(roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply non-maximum supression threshold > 0.7, It is the process in which we remove/merge extremely highly overlapping bounding boxes. \n",
    "\n",
    "We keep the threshold at 0.7. threshold defines the minimum overlapping area required to merge/remove overlapping bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seudo code\n",
    "\n",
    "- Take all the roi boxes [roi_array]\n",
    "- Find the areas of all the boxes [roi_area]\n",
    "- Take the indexes of order the probability score in descending order [order_array]\n",
    "keep = []\n",
    "while order_array.size > 0:\n",
    "  - take the first element in order_array and append that to keep  \n",
    "  - Find the area with all other boxes\n",
    "  - Find the index of all the boxes which have high overlap with this box\n",
    "  - Remove them from order array\n",
    "  - Iterate this till we get the order_size to zero (while loop)\n",
    "- Ouput the keep variable which tells what indexes to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roi contains the first top 12000 anchors\n",
    "# roi format: [y1,x1,y2,x2]\n",
    "y1 = roi[:, 0]\n",
    "x1 = roi[:, 1]\n",
    "y2 = roi[:, 2]\n",
    "x2 = roi[:, 3]\n",
    "\n",
    "area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "order = score[order].argsort()[::-1]\n",
    "\n",
    "keep = []\n",
    "\n",
    "while order.size > 0:\n",
    "    i = order[0]\n",
    "    keep.append(i)\n",
    "    # Compare the first in the order aginst all the other anchors\n",
    "    xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "    yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "    xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "    yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "   \n",
    "    # calculate the area of intersection\n",
    "    w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "    h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "    inter = w * h\n",
    "    \n",
    "    # calculate the IOU between the area of the first anchor vs the others\n",
    "    ovr = inter / (area[i] + area[order[1:]] - inter)\n",
    "    \n",
    "    # Discard the anchors which overlap more than threshold \"nms_thresh\"\n",
    "    inds = np.where(ovr <= nms_thresh)[0]\n",
    "    # Take the new order to continue with the algorithm\n",
    "    order = order[inds + 1]\n",
    "    \n",
    "# Just choose the indexes of the anchors for the training \"n_train_post_nms\"    \n",
    "keep = keep[:n_train_post_nms]\n",
    "\n",
    "# the final region proposals (2000 proposals in this case)\n",
    "roi = roi[keep] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the proposal targets: (612, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the proposal targets: {}\".format(roi.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final region proposals were obtained, this is used as the input to the Fast_R-CNN object which finally tries to predict the object locations and class of the object. First we look into how to create targets for these proposals for training this network. After that we will look into how this fast r-cnn network is implemented and pass these proposals to the network to obtain the predicted outputs. Then, we will determine the losses, We will calculate both the rpn loss and fast r-cnn loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fast R-CNN network takes the region proposals (obtained from proposal layer in previous section), ground truth boxes and their respective labels as inputs. It will take the following parameters.\n",
    "\n",
    "n_sample: Number of samples to sample from roi, The default value is 128.\n",
    "\n",
    "pos_ratio: the number of positive examples out of the n_samples. The default values is 0.25.\n",
    "\n",
    "pos_iou_thesh: The minimum overlap of region proposal with any groundtruth object to consider it as positive label.\n",
    "\n",
    "[neg_iou_threshold_lo, neg_iou_threshold_hi] : [0.0, 0.5], The overlap value bounding required to consider a region proposal as negitive [background object]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples for the training\n",
    "n_sample = 128\n",
    "# ratio of positive examples\n",
    "pos_ratio = 0.25\n",
    "# threshold to consider the proposal as object\n",
    "pos_iou_thresh = 0.5\n",
    "# thresholds to consider the proposal as non-object\n",
    "neg_iou_thresh_hi = 0.5\n",
    "neg_iou_thresh_lo = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these params, lets see how the proposal targets are created, First lets write the sudo code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each roi, find the IoU with all other ground truth object [N, n]\n",
    "    - where N is the number of region proposal boxes\n",
    "    - n is the number of ground truth boxes\n",
    "- Find which ground truth object has highest iou with the roi [N], these are the labels for each and every region proposal\n",
    "- If the highest IoU is greater than pos_iou_thesh[0.5], then we assign the label.\n",
    "- pos_samples:\n",
    "      - We randomly samply [n_sample x pos_ratio] region proposals and consider these only as positive labels\n",
    "- If the IoU is between [0.1, 0.5], we assign a negitive label[0] to the region proposal\n",
    "- neg_samples:\n",
    "      - We randomly sample [128- number of pos region proposals on this image] and assign 0 to these region proposals\n",
    "- We collect the pos_samples and neg_samples  and remove all other region proposals\n",
    "- convert the locations of groundtruth objects for each region proposal to the required format (Described in Fast R-CNN)\n",
    "- Ouput labels and locations for the sampled_rois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the iou of each ground truth object with the region proposals, We will use the same code we have used in Anchor boxes to calculate the ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612, 2)\n"
     ]
    }
   ],
   "source": [
    "ious = np.empty((len(roi), 2), dtype=np.float32)\n",
    "ious.fill(0)\n",
    "for num1, i in enumerate(roi):\n",
    "    ya1, xa1, ya1, xa2 = i\n",
    "    anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
    "    for num2, j in enumerate(bbox):\n",
    "        yb1, xb1, yb2, xb2 = j\n",
    "        box_area = (yb2 - yb1) * (xb2 - xb1)\n",
    "        \n",
    "        inter_x1 = max([xb1, xa1])\n",
    "        inter_y1 = max([yb1, ya1])\n",
    "        inter_x2 = min([xb2, xa2])\n",
    "        inter_y2 = min([yb2, ya2])\n",
    "        \n",
    "        if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
    "            inter_area = (inter_y2 - inter_y1) * (inter_x2 - inter_x1)\n",
    "            iou = iter_area / (anchor_area + box_area - iter_area)\n",
    "        else:\n",
    "            iou = 0.\n",
    "            \n",
    "        ious[num1, num2] = iou\n",
    "print(ious.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out which ground truth has high IoU for each region proposal, Also find the maximum IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1\n",
      " 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0\n",
      " 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1\n",
      " 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0\n",
      " 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1\n",
      " 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "[0.         0.002455   0.         0.         0.         0.0032808\n",
      " 0.         0.00276414 0.         0.         0.         0.\n",
      " 0.         0.00218318 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00205604 0.00281916 0.00417052 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00224674 0.0021169  0.00355105\n",
      " 0.         0.00266993 0.         0.         0.00266993 0.00232314\n",
      " 0.00455198 0.         0.00242664 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.0038784  0.\n",
      " 0.         0.         0.         0.         0.00316501 0.\n",
      " 0.00531595 0.0030372  0.         0.         0.00300654 0.\n",
      " 0.         0.         0.         0.00371535 0.         0.\n",
      " 0.         0.         0.00248449 0.         0.         0.\n",
      " 0.         0.00554194 0.00497009 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00207322 0.00202444 0.         0.\n",
      " 0.         0.         0.         0.00220638 0.         0.\n",
      " 0.         0.         0.00242403 0.00226447 0.00235758 0.00249434\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00215119 0.         0.         0.00377002 0.00334467 0.00334467\n",
      " 0.00334467 0.         0.         0.         0.         0.00536194\n",
      " 0.00345875 0.00358104 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00385363 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.0041712  0.         0.         0.         0.         0.00400613\n",
      " 0.         0.         0.         0.         0.0041712  0.00666974\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00193099 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00222095\n",
      " 0.00212212 0.         0.         0.         0.         0.\n",
      " 0.         0.00229211 0.00200326 0.         0.00206098 0.\n",
      " 0.00189702 0.00308347 0.00308347 0.00364453 0.00308347 0.00189703\n",
      " 0.00308347 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00194872 0.00315143 0.00315143\n",
      " 0.00315143 0.         0.00192254 0.00315143 0.00315143 0.00322241\n",
      " 0.00322241 0.00194872 0.00322241 0.00380194 0.00322241 0.\n",
      " 0.         0.         0.00455658 0.00363137 0.         0.00428372\n",
      " 0.         0.00382556 0.         0.0040417  0.         0.\n",
      " 0.00486656 0.00382556 0.         0.0052218  0.         0.\n",
      " 0.         0.00428372 0.         0.00354148 0.         0.0040417\n",
      " 0.         0.00337442 0.         0.         0.         0.\n",
      " 0.         0.         0.0052218  0.00486656 0.         0.\n",
      " 0.         0.         0.         0.         0.00337442 0.\n",
      " 0.         0.         0.         0.         0.         0.0052218\n",
      " 0.00372594 0.00455658 0.         0.         0.         0.00354148\n",
      " 0.         0.00486656 0.         0.00455658 0.00428372 0.\n",
      " 0.0040417  0.00382556 0.         0.         0.         0.\n",
      " 0.00469588 0.         0.         0.00499773 0.         0.\n",
      " 0.         0.00415919 0.         0.00393066 0.00534105 0.\n",
      " 0.00369235 0.00387644 0.         0.         0.00337442 0.00573502\n",
      " 0.00442842 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00372593 0.00345593 0.         0.00345593\n",
      " 0.0054196  0.0054196  0.         0.         0.0052218  0.\n",
      " 0.00372594 0.00486656 0.         0.         0.00393066 0.\n",
      " 0.         0.         0.         0.         0.         0.00455658\n",
      " 0.00470647 0.00393066 0.00415919 0.00438359 0.00441594 0.00503793\n",
      " 0.0054196  0.         0.         0.00503793 0.         0.00470647\n",
      " 0.00441594 0.00415919 0.00393066 0.         0.         0.\n",
      " 0.00496527 0.         0.0055045  0.0054196  0.00337442 0.00441594\n",
      " 0.00582056 0.         0.         0.         0.         0.\n",
      " 0.005221   0.         0.00363137 0.         0.00428372 0.\n",
      " 0.         0.0054196  0.00337442 0.         0.         0.00345593\n",
      " 0.         0.         0.00354148 0.00232947 0.00372594 0.\n",
      " 0.         0.00382556 0.         0.         0.         0.00627919\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.0052218  0.         0.00212216\n",
      " 0.00415919 0.         0.00470647 0.00441594 0.         0.\n",
      " 0.00503793 0.         0.         0.         0.0040417  0.00354148\n",
      " 0.00354148 0.00617512 0.0040417  0.00382556 0.00503793 0.00455658\n",
      " 0.         0.         0.00486656 0.00218704 0.         0.00203175\n",
      " 0.00225601 0.         0.         0.         0.         0.00229215\n",
      " 0.         0.         0.         0.00329667 0.00197564 0.00329667\n",
      " 0.00329667 0.00336851 0.         0.00209114 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.0018481  0.00295624\n",
      " 0.00295624 0.00295624 0.00316598 0.00295624 0.00295624 0.\n",
      " 0.         0.         0.         0.         0.         0.00187236\n",
      " 0.00301882 0.         0.         0.         0.00531506 0.00570797\n",
      " 0.         0.00407142 0.00307823 0.         0.         0.\n",
      " 0.00264534 0.00531506 0.         0.00407142 0.00264534 0.\n",
      " 0.         0.         0.00222993 0.00222993 0.00407142 0.00222993\n",
      " 0.         0.         0.00264534 0.00570797 0.00264534 0.00264534\n",
      " 0.         0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Which is the the maximum iou of every ground_truth\n",
    "gt_assignment = ious.argmax(axis=1)\n",
    "max_iou = ious.max(axis=1)\n",
    "print(gt_assignment)\n",
    "print(max_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the labels to each proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 8 6 6 6 8 6 8 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 8 8 8 6 6 6 6 6 6 6 6 6 6 8 8 8 6 8\n",
      " 6 6 8 8 8 6 8 6 6 6 6 6 6 6 8 6 6 6 6 6 8 6 8 8 6 6 8 6 6 6 6 8 6 6 6 6 8\n",
      " 6 6 6 6 8 8 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 8 8 8 8 6 6 6 6 8 8 8 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 8 6 6 6 6 6 6 6 6 6 6 6 8 6 6 6 6 8 6 6 6 6 8 8 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 8\n",
      " 8 8 8 6 8 6 6 6 6 6 6 6 6 6 8 8 8 6 6 8 8 8 8 6 8 8 8 6 6 6 8 8 6 8 6 8 6\n",
      " 8 6 6 8 8 6 8 6 6 6 8 6 8 6 8 6 8 6 6 6 6 6 6 8 8 6 6 6 6 6 6 8 6 6 6 6 6\n",
      " 6 8 8 8 6 6 6 8 6 8 6 8 8 6 8 8 6 6 6 6 8 6 6 8 6 6 6 8 6 8 8 6 8 8 6 6 8\n",
      " 8 8 6 6 6 6 6 6 6 8 8 6 8 8 8 6 6 8 6 8 8 6 6 8 6 6 6 6 6 6 8 8 8 8 8 8 8\n",
      " 8 6 6 8 6 8 8 8 8 6 6 6 8 6 8 8 8 8 8 6 6 6 6 6 8 6 8 6 8 6 6 8 8 6 6 8 6\n",
      " 6 8 6 8 6 6 8 6 6 6 8 6 6 6 6 6 6 6 6 6 8 6 6 8 6 8 8 6 6 8 6 6 6 8 8 8 8\n",
      " 8 8 8 8 6 6 8 6 6 6 6 6 6 6 6 6 6 6 6 8 6 8 8 8 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 8 8 8 8 8 8 6 6 6 6 6 6 6 8 6 6 6 8 8 6 8 8 6 6 6 6 8 6 8\n",
      " 6 6 6 6 6 6 8 6 6 6 6 8 6 6 6 6 6 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "gt_roi_label = labels[gt_assignment]\n",
    "print(gt_roi_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Incase if u have not taken the background object as 0, add +1 to all the labels.\n",
    "\n",
    "Select the foreground rois as per the pos_iou_thesh. We also want only n_sample x pos_ratio (128 x 0.25 = 32) foreground samples. So incase if we get less than 32 positive samples we will leave it as it is, Incase if we get more than 32 foreground samples, we will sample 32 samples from the positive samples. This is done using the following code.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# choose the positive indexes\n",
    "pos_index = np.where(max_iou >= pos_iou_thresh)[0]\n",
    "pos_roi_per_image = int(n_sample * pos_ratio)\n",
    "\n",
    "pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
    "if pos_index.size > 0:\n",
    "    pos_index = np.random.choice(pos_index, size=pos_roi_per_this_image, replace=False)\n",
    "print(pos_roi_per_this_image)\n",
    "print(pos_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we do for negitive (background) region proposals also, If we have region proposals with IoU between neg_iou_thresh_lo and neg_iou_thresh_hi for the ground truth object assigned to it earlier, we assign 0 label to the region proposal. We will sample n(n_sample-pos_samples, 128–32=96) region proposals from these negitive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "[423 117 375  82 331  54 530 401 291 464 204 306 406 398 136  24 164 508\n",
      " 476 241 429 387 510 383 440 237 583 273 498 339 328 145 495  57 369 558\n",
      " 524 560 355 187 157  84 607 116 199 148 471 362 377 181  87 419 425  16\n",
      " 299 131 340 124 489 200  32 463  71 221 611 470 206 186 574 135 354 479\n",
      " 316 229 368 261 185 522 191 579  68 461  18  23 119 144  12 516 253 184\n",
      " 337 222 359 374 428 309 580  46 126 166 233 552 578 586  65 467 477 568\n",
      " 268 258 486 242  53 171   9 134 422  35 408 388 512 321 244 450 361 572\n",
      " 211 165]\n"
     ]
    }
   ],
   "source": [
    "neg_index = np.where((max_iou < neg_iou_thresh_hi) & (max_iou >= neg_iou_thresh_lo))[0]\n",
    "neg_roi_per_this_image = n_sample - pos_roi_per_this_image\n",
    "neg_roi_per_this_image = int(min(neg_roi_per_this_image, neg_index.size))\n",
    "if neg_index.size > 0:\n",
    "    neg_index = np.random.choice(neg_index, size=neg_roi_per_this_image, replace=False)\n",
    "\n",
    "print(neg_roi_per_this_image)\n",
    "print(neg_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we gather positve samples index and negitive samples index, their respective labels and region proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 4)\n",
      "[[554.5799   381.3491   703.83484  679.8565  ]\n",
      " [269.2232   594.3313   567.6415   800.      ]\n",
      " [474.57993  173.3491   623.83484  471.85654 ]\n",
      " [477.22195   82.32638  775.6396   679.2876  ]\n",
      " [170.57993  205.3491   319.8348   503.85654 ]\n",
      " [397.20966  626.2786   695.6208   800.      ]\n",
      " [634.5799   237.3491   783.83484  535.8565  ]\n",
      " [282.57993  621.3491   431.8348   800.      ]\n",
      " [362.57013   13.359273 511.82968  311.87567 ]\n",
      " [538.5799   269.3491   687.83484  567.8565  ]\n",
      " [ 68.76552  309.07184  491.38586  730.94476 ]\n",
      " [682.5774   125.35173  800.       423.86148 ]\n",
      " [ 90.579926 205.3491   239.83482  503.85654 ]\n",
      " [394.57993  621.3491   543.83484  800.      ]\n",
      " [164.73434    0.       587.44086  410.93036 ]\n",
      " [733.15265   50.05752  800.       647.0857  ]\n",
      " [644.756    245.07704  800.       666.94037 ]\n",
      " [362.57993  285.3491   511.8348   583.8565  ]\n",
      " [ 90.579926 285.3491   239.83482  583.8565  ]\n",
      " [682.5608   733.36896  800.       800.      ]\n",
      " [554.5799   621.3491   703.83484  800.      ]\n",
      " [250.57993  621.3491   399.8348   800.      ]\n",
      " [314.57993  285.3491   463.8348   583.8565  ]\n",
      " [442.57993  173.3491   591.83484  471.85654 ]\n",
      " [218.57993  381.3491   367.8348   679.8565  ]\n",
      " [660.86725  101.01634  800.       522.99176 ]\n",
      " [  0.       382.48184  177.51816  593.5182  ]\n",
      " [442.56766  685.3618   591.82837  800.      ]\n",
      " [602.5799   333.3491   751.83484  631.8565  ]\n",
      " [330.57993  205.3491   479.8348   503.85654 ]\n",
      " [ 90.579926 621.3491   239.83482  800.      ]\n",
      " [490.35754    0.       639.71844  248.29073 ]\n",
      " [602.5799   237.3491   751.83484  535.8565  ]\n",
      " [  0.       386.30856  295.6326   800.      ]\n",
      " [586.5799   461.3491   735.83484  759.8565  ]\n",
      " [362.58878  717.33997  511.83945  800.      ]\n",
      " [298.57993  333.3491   447.8348   631.8565  ]\n",
      " [426.58878  717.33997  575.8395   800.      ]\n",
      " [538.5799   205.3491   687.83484  503.85654 ]\n",
      " [452.7655   181.07184  800.       602.94476 ]\n",
      " [  0.       210.65819  800.       800.      ]\n",
      " [477.22195  530.32635  775.6396   800.      ]\n",
      " [701.2845   474.64224  800.       623.86743 ]\n",
      " [ 45.222218 578.3274   343.63998  800.      ]\n",
      " [180.76552  117.07184  603.38586  538.94476 ]\n",
      " [186.35756    0.       335.7184   248.29073 ]\n",
      " [570.5799    93.3491   719.83484  391.85654 ]\n",
      " [506.57993  173.3491   655.83484  471.85654 ]\n",
      " [122.579926 477.3491   271.8348   775.8565  ]\n",
      " [340.7655   229.07184  763.38586  650.94476 ]\n",
      " [269.22195  210.32637  567.6396   800.      ]\n",
      " [106.579926 445.3491   255.83482  743.8565  ]\n",
      " [618.5799   365.3491   767.83484  663.8565  ]\n",
      " [324.05548    0.       748.6362   346.61835 ]\n",
      " [ 26.575357  93.35384  175.83243  391.86545 ]\n",
      " [269.83984  626.66846  800.       800.      ]\n",
      " [554.5799   141.3491   703.83484  439.85654 ]\n",
      " [333.89728  674.67664  800.       800.      ]\n",
      " [554.5799   301.3491   703.83484  599.8565  ]\n",
      " [324.7655   293.07184  747.38586  714.94476 ]\n",
      " [708.5455   133.19167  800.       554.8433  ]\n",
      " [442.57993  589.3491   591.83484  800.      ]\n",
      " [ 29.221409 466.3243   327.63876  800.      ]\n",
      " [244.76619  613.0715   667.3847   800.      ]\n",
      " [701.2845    50.568916 800.       647.4697  ]\n",
      " [138.57993  285.3491   287.8348   583.8565  ]\n",
      " [516.7655   229.07184  800.       650.94476 ]\n",
      " [372.7655   149.07184  795.38586  570.94476 ]\n",
      " [714.6119   333.31592  800.       631.7941  ]\n",
      " [  0.         0.       143.70912  248.32544 ]\n",
      " [362.57993  397.3491   511.8348   695.8565  ]\n",
      " [106.579926 365.3491   255.83482  663.8565  ]\n",
      " [ 42.57853  285.35056  191.83409  583.85925 ]\n",
      " [762.51086  125.42078  800.       423.99136 ]\n",
      " [570.5799   173.3491   719.83484  471.85654 ]\n",
      " [  0.       701.36566  143.82646  800.      ]\n",
      " [132.76552  421.07184  555.38586  800.      ]\n",
      " [586.5799    61.3491   735.83484  359.85654 ]\n",
      " [196.76552  197.07184  619.38586  618.94476 ]\n",
      " [734.4818   126.48184  800.       337.51816 ]\n",
      " [  0.       709.12366  363.55347  800.      ]\n",
      " [250.57993  125.3491   399.8348   423.85654 ]\n",
      " [500.05548    0.       800.       346.61835 ]\n",
      " [733.15265  562.0575   800.       800.      ]\n",
      " [205.22322  594.3313   503.6415   800.      ]\n",
      " [378.35754    0.       527.71844  248.29073 ]\n",
      " [  0.         0.       800.       487.72818 ]\n",
      " [122.579926 109.3491   271.8348   407.85654 ]\n",
      " [314.56155  733.36816  463.8252   800.      ]\n",
      " [260.7655   421.07184  683.38586  800.      ]\n",
      " [170.57993  413.3491   319.8348   711.8565  ]\n",
      " [164.76619  613.0715   587.3847   800.      ]\n",
      " [602.5799   157.3491   751.83484  455.85654 ]\n",
      " [122.579926 621.3491   271.8348   800.      ]\n",
      " [650.5799   365.3491   799.83484  663.8565  ]\n",
      " [ 58.577827  29.351278 207.83372  327.86063 ]\n",
      " [  0.       365.28445  487.46976  663.73486 ]\n",
      " [669.2058   626.2637   800.       800.      ]\n",
      " [372.70703  709.1037   795.489    800.      ]\n",
      " [701.2396   674.3949   800.       800.      ]\n",
      " [762.51086  509.42078  800.       800.      ]\n",
      " [186.58876  717.33997  335.83945  800.      ]\n",
      " [746.63086    0.       800.       295.7571  ]\n",
      " [717.2845   154.64223  800.       303.86743 ]\n",
      " [  0.       674.69196  800.       800.      ]\n",
      " [506.57993  269.3491   655.83484  567.8565  ]\n",
      " [618.5799    61.3491   767.83484  359.85654 ]\n",
      " [  0.       205.33328  143.8428   503.82678 ]\n",
      " [570.5676   685.3618   719.82837  800.      ]\n",
      " [ 26.561634 685.3681   175.82524  800.      ]\n",
      " [650.5799    77.3491   799.83484  375.85654 ]\n",
      " [ 74.56154  733.3682   223.8252   800.      ]\n",
      " [333.20966  626.2786   631.6208   800.      ]\n",
      " [285.72437  178.6521   800.       775.5223  ]\n",
      " [220.99889    0.       519.2995   486.6375  ]\n",
      " [ 26.335726   0.       175.707    248.33336 ]\n",
      " [586.5799   365.3491   735.83484  663.8565  ]\n",
      " [708.5455   213.19167  800.       634.8433  ]\n",
      " [218.57993  525.3491   367.8348   800.      ]\n",
      " [426.57993  413.3491   575.83484  711.8565  ]\n",
      " [618.5799   557.3491   767.83484  800.      ]\n",
      " [ 58.57939  557.3497   207.83455  800.      ]\n",
      " [122.56156  733.36816  271.8252   800.      ]\n",
      " [250.57993  445.3491   399.8348   743.8565  ]\n",
      " [458.57993  461.3491   607.83484  759.8565  ]\n",
      " [714.6119   141.31593  800.       439.7941  ]\n",
      " [116.76552  181.07184  539.38586  602.94476 ]\n",
      " [644.756    165.07704  800.       586.94037 ]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "keep_index = np.append(pos_index, neg_index)\n",
    "# Assign labels\n",
    "gt_roi_labels = gt_roi_label[keep_index]\n",
    "# neg samples labeled as 0\n",
    "gt_roi_labels[pos_roi_per_this_image:] = 0\n",
    "# Sample the roi proposals # sample_roi format = roi format: [y1,x1,y2,x2]\n",
    "sample_roi = roi[keep_index]\n",
    "print(sample_roi.shape)\n",
    "print(sample_roi)\n",
    "print(gt_roi_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the ground truth objects for these sample_roi and later parameterize as we have done while assigning locations to anchor boxes in section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 4)\n"
     ]
    }
   ],
   "source": [
    "# Get the relative diference between the roi proposed and their most proximity ground truth box\n",
    "bbox_for_sampled_roi = bbox[gt_assignment[keep_index]]\n",
    "print(bbox_for_sampled_roi.shape)\n",
    "\n",
    "height = sample_roi[:, 2] - sample_roi[:, 0]\n",
    "width = sample_roi[:, 3] - sample_roi[:, 0]\n",
    "ctr_y = sample_roi[:, 0] + (0.5 * height)\n",
    "ctr_x = sample_roi[:, 1] + (0.5 * width)\n",
    "\n",
    "base_height = bbox_for_sampled_roi[:, 2] - bbox_for_sampled_roi[:, 0]\n",
    "base_width = bbox_for_sampled_roi[:, 3] - bbox_for_sampled_roi[:, 1]\n",
    "base_ctr_y = bbox_for_sampled_roi[:, 0] + (0.5 * base_height)\n",
    "base_ctr_x = bbox_for_sampled_roi[:, 1]  + (0.5 * base_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.80866671e+00 -1.42873752e+00  9.34515357e-01  1.32220852e+00]\n",
      " [-6.98457062e-01 -1.12047052e+00  2.41675183e-01 -1.21608861e-01]\n",
      " [-2.27267146e+00  7.80246144e+08  9.34515536e-01  2.20951176e+01]\n",
      " [-1.39546311e+00  4.04031128e-01  2.41677254e-01  8.44140112e-01]\n",
      " [ 1.03710270e+00  3.84103090e-01  2.92661905e-01 -5.10655403e-01]\n",
      " [-1.12735474e+00 -1.39693975e+00  2.41699055e-01  1.54316470e-01]\n",
      " [-3.34466243e+00  6.46028416e+08  9.34515357e-01  2.20951176e+01]\n",
      " [-9.86281753e-01 -1.18870378e+00  9.34515834e-01 -9.61223096e-02]\n",
      " [-1.52217984e+00  2.32354330e+09  9.34484541e-01  2.20951176e+01]\n",
      " [-2.70146751e+00 -6.48551702e-01  9.34515357e-01  2.77594376e+00]\n",
      " [ 2.83763707e-01 -2.11667001e-01 -7.48156905e-01 -1.19721901e+00]\n",
      " [-4.52458620e+00  2.25658778e+09  1.17439175e+00  2.20951176e+01]\n",
      " [ 1.57309830e+00  2.12962866e-01  2.92661816e-01 -7.25799739e-01]\n",
      " [-1.73667586e+00 -1.37896264e+00  9.34515536e-01  1.47808939e-01]\n",
      " [-3.92914653e-01  5.76378107e-01 -1.06506869e-01  6.46604657e-01]\n",
      " [-8.32607841e+00  2.16405914e+09  1.73775959e+00  2.20951176e+01]\n",
      " [-3.30046844e+00  3.98061454e-01  8.95173073e-01  3.05334401e+00]\n",
      " [-1.52227759e+00 -5.91962159e-01  9.34515834e-01  7.53319323e-01]\n",
      " [ 1.57309830e+00 -6.48467168e-02  2.92661816e-01 -9.02752697e-01]\n",
      " [-4.52387571e+00 -4.48818207e+00  1.17425036e+00  1.38681185e+00]\n",
      " [-2.80866671e+00 -1.95199656e+00  9.34515357e-01  6.49761260e-01]\n",
      " [-7.71883428e-01 -1.14859140e+00  9.34515834e-01 -1.56130582e-01]\n",
      " [ 7.23100826e-02  2.97139227e-01  2.92661905e-01 -2.97421694e-01]\n",
      " [-2.05827332e+00  2.63051581e+00  9.34515536e-01  2.77594376e+00]\n",
      " [ 7.15505123e-01 -2.42777050e-01  2.92661905e-01 -8.35680425e-01]\n",
      " [-3.74055409e+00  1.95388634e+09  1.00474274e+00  2.20951176e+01]\n",
      " [ 1.75329053e+00 -3.01997423e-01  1.19244479e-01 -1.08775043e+00]\n",
      " [-2.05813026e+00 -1.67605984e+00  9.34476793e-01  2.73786575e-01]\n",
      " [-3.13026404e+00 -2.83459711e+00  9.34515357e-01  2.77594376e+00]\n",
      " [-3.48890908e-02  1.20046532e+00  2.92661905e-01  1.43428192e-01]\n",
      " [ 3.00108284e-01 -1.00231051e+00  9.34515715e-01 -4.11715060e-01]\n",
      " [-2.37704754e+00  3.23828301e+09  9.33805764e-01  2.20951176e+01]\n",
      " [-3.13026404e+00  5.11810688e+08  9.34515357e-01  2.20951176e+01]\n",
      " [ 8.53030741e-01 -3.57885748e-01 -3.90800059e-01 -1.38629436e+00]\n",
      " [-3.02306509e+00 -1.63315415e+00  9.34515357e-01  9.97843504e-01]\n",
      " [-1.52236581e+00 -1.53412974e+00  9.34544027e-01  7.18589574e-02]\n",
      " [ 1.79509252e-01  3.79092853e-05  2.92661905e-01 -5.10655344e-01]\n",
      " [-1.95117414e+00 -1.71137226e+00  9.34543848e-01  2.30052367e-01]\n",
      " [-2.70146751e+00  6.46028288e+08  9.34515357e-01  2.20951176e+01]\n",
      " [-1.19913995e+00  5.88532090e-02  9.01709124e-02  1.14090312e+00]\n",
      " [-2.37499997e-01 -4.32072759e-01 -7.44440496e-01 -5.31879067e-01]\n",
      " [-1.39546311e+00 -1.32200873e+00  2.41677254e-01  3.75767767e-01]\n",
      " [-5.47677040e+00 -1.43389594e+09  1.34792912e+00  2.20951176e+01]\n",
      " [ 6.88862860e-01 -6.03775501e-01 -4.00176972e-01 -1.32810605e+00]\n",
      " [-4.30825621e-01 -8.69996473e-02 -1.06303006e-01  2.71699071e-01]\n",
      " [-3.41709316e-01  3.77880597e+00  9.33806002e-01  2.02667689e+00]\n",
      " [-2.91586590e+00  2.18953216e+09  9.34515357e-01  2.20951176e+01]\n",
      " [-2.48706985e+00  9.14463872e+08  9.34515536e-01  2.20951176e+01]\n",
      " [ 1.35870016e+00 -4.65327263e-01  2.92661905e-01 -1.18368328e+00]\n",
      " [-8.09415996e-01 -3.84169698e-01 -1.06303006e-01  4.15582299e-01]\n",
      " [-6.98453307e-01 -3.96993399e-01  2.41677254e-01 -1.21611290e-01]\n",
      " [ 1.46589923e+00 -4.14243072e-01  2.92661816e-01 -1.15888643e+00]\n",
      " [-3.23746324e+00 -2.71635604e+00  9.34515357e-01  2.33994198e+00]\n",
      " [-7.68630743e-01  1.12449617e+01 -1.10930979e-01  3.03642726e+00]\n",
      " [ 7.28917658e-01 -3.01100444e-02  9.34501112e-01  2.52040863e-01]\n",
      " [-6.12871230e-01 -1.18218720e+00 -3.33007932e-01 -1.20446451e-01]\n",
      " [-2.80866671e+00  1.51844352e+09  9.34515357e-01  2.20951176e+01]\n",
      " [-7.65815318e-01 -1.37894070e+00 -2.04234824e-01  8.32669530e-03]\n",
      " [-2.80866671e+00 -1.30282271e+00  9.34515357e-01  2.33994198e+00]\n",
      " [-7.71556973e-01 -5.71946025e-01 -1.06303006e-01  1.86126456e-01]\n",
      " [-5.95129299e+00  1.75036198e+09  1.42432940e+00  2.20951176e+01]\n",
      " [-2.05827332e+00 -1.40747309e+00  9.34515536e-01  2.73820937e-01]\n",
      " [ 7.42483318e-01 -4.56309557e-01 -4.00175661e-01 -1.34908378e+00]\n",
      " [-5.82263708e-01 -1.12689161e+00 -1.06298693e-01 -1.66656643e-01]\n",
      " [-5.47677040e+00  2.02449382e+09  1.34792912e+00  2.20951176e+01]\n",
      " [ 1.25150096e+00 -1.79379880e-02  2.92661905e-01 -8.00378263e-01]\n",
      " [-1.58307958e+00 -2.32237592e-01  2.93896109e-01  1.25355601e+00]\n",
      " [-8.85134101e-01  8.49661604e-02 -1.06303006e-01  8.63560677e-01]\n",
      " [-6.40962553e+00 -2.25712640e+08  1.49296427e+00  2.20951176e+01]\n",
      " [ 9.61285174e-01  5.67148030e-01  9.72379982e-01  6.37992561e-01]\n",
      " [-1.52227759e+00 -8.97114813e-01  9.34515834e-01  3.43759954e-01]\n",
      " [ 1.46589923e+00 -2.58376956e-01  2.92661816e-01 -1.02474439e+00]\n",
      " [ 1.89469469e+00 -1.03441536e-01  2.92657375e-01 -9.95620668e-01]\n",
      " [-1.52378931e+01  2.59072922e+09  2.31612015e+00  2.20951176e+01]\n",
      " [-2.91586590e+00  1.18289920e+09  9.34515357e-01  2.20951176e+01]\n",
      " [ 9.60092902e-01 -1.04545712e+00  9.71563816e-01 -5.31879067e-01]\n",
      " [-3.17248523e-01 -7.33908534e-01 -1.06303006e-01 -3.50408912e-01]\n",
      " [-3.02306509e+00  2.65929421e+09  9.34515357e-01  2.20951176e+01]\n",
      " [-4.68684673e-01 -3.39101136e-01 -1.06303006e-01  1.07302636e-01]\n",
      " [-8.50513268e+00  2.82696090e+09  1.75784349e+00  2.20951176e+01]\n",
      " [ 7.76316822e-02 -1.05515456e+00  4.42448556e-02 -5.31879067e-01]\n",
      " [ 5.01106799e-01  1.66215515e+00  2.92661905e-01  1.43428192e-01]\n",
      " [-1.46703029e+00  2.86654310e+09  2.36573771e-01  2.20951176e+01]\n",
      " [-8.32607841e+00 -4.94381905e+00  1.73775959e+00  1.95032108e+00]\n",
      " [-4.83993053e-01 -1.05370569e+00  2.41675183e-01 -2.35453516e-01]\n",
      " [-1.62718618e+00  2.76852096e+09  9.33805764e-01  2.20951176e+01]\n",
      " [-2.37499997e-01  4.33354303e-02 -7.44440496e-01 -3.70255262e-02]\n",
      " [ 1.35870016e+00  8.69375885e-01  2.92661905e-01 -3.55141908e-01]\n",
      " [-1.20051575e+00 -1.46483541e+00  9.34457123e-01 -3.23197730e-02]\n",
      " [-6.20120823e-01 -7.89432228e-01 -1.06303006e-01 -1.37417868e-01]\n",
      " [ 1.03710270e+00 -3.39913875e-01  2.92661905e-01 -9.95613039e-01]\n",
      " [-3.92967731e-01 -1.04794228e+00 -1.06298693e-01 -3.01260471e-01]\n",
      " [-3.13026404e+00  1.51844352e+09  9.34515357e-01  2.20951176e+01]\n",
      " [ 8.57099816e-02 -1.02603865e+00  9.34515834e-01 -3.65558863e-01]\n",
      " [-3.45186162e+00 -8.05833626e+00  9.34515357e-01  3.56672883e+00]\n",
      " [ 5.14513850e-01  3.75097573e-01  9.34509099e-01  5.56970537e-01]\n",
      " [ 3.20563734e-01 -2.97034055e-01 -8.90910864e-01 -1.19956541e+00]\n",
      " [-4.01090384e+00 -3.26207733e+00  1.06654620e+00  1.27910769e+00]\n",
      " [-8.84848535e-01 -1.53934240e+00 -1.06685370e-01  9.52628404e-02]\n",
      " [-5.47405577e+00 -4.64533567e+00  1.34747481e+00  1.56003618e+00]\n",
      " [-1.52378931e+01 -7.01977634e+00  2.31612015e+00  2.52868152e+00]\n",
      " [-3.43141556e-01 -1.23741722e+00  9.34543908e-01 -2.66302943e-01]\n",
      " [-1.05550776e+01  4.11408282e+09  1.96293855e+00  2.20951176e+01]\n",
      " [-6.63288164e+00  2.65974477e+09  1.52476406e+00  2.20951176e+01]\n",
      " [-2.37499997e-01 -1.01211488e+00 -7.44440496e-01 -5.31879067e-01]\n",
      " [-2.48706985e+00 -5.70974529e-01  9.34515536e-01  2.03733492e+00]\n",
      " [-3.23746324e+00  2.79351194e+09  9.34515357e-01  2.20951176e+01]\n",
      " [ 2.28081346e+00  8.48571956e-02  3.29596311e-01 -9.23915148e-01]\n",
      " [-2.91568971e+00 -2.33218169e+00  9.34476495e-01  7.17124403e-01]\n",
      " [ 7.28955746e-01 -1.04350567e+00  9.34457302e-01 -4.98113275e-01]\n",
      " [-3.45186162e+00  2.72640307e+09  9.34515357e-01  2.20951176e+01]\n",
      " [ 4.07377392e-01 -1.14563453e+00  9.34457004e-01 -4.34043586e-01]\n",
      " [-9.12885606e-01 -1.27396345e+00  2.41699055e-01  6.85246009e-03]\n",
      " [-6.47244692e-01 -3.23707074e-01 -3.02588165e-01 -4.12602052e-02]\n",
      " [-5.36871791e-01  4.97595936e-01  2.42069513e-01  5.70595920e-01]\n",
      " [ 7.29582310e-01  6.93706453e-01  9.33736324e-01  7.50065923e-01]\n",
      " [-3.02306509e+00 -1.79857004e+00  9.34515357e-01  1.80534136e+00]\n",
      " [-5.95129299e+00  7.43729024e+08  1.42432940e+00  2.20951176e+01]\n",
      " [ 7.15505123e-01 -5.43598652e-01  2.92661905e-01 -1.06715608e+00]\n",
      " [-1.95107412e+00 -1.02001846e+00  9.34515536e-01  4.99273568e-01]\n",
      " [-3.23746324e+00 -2.11144853e+00  9.34515357e-01  9.51917291e-01]\n",
      " [ 1.78749621e+00 -5.77351034e-01  2.92660028e-01 -1.31025076e+00]\n",
      " [ 8.57985914e-02 -1.19138122e+00  9.34457123e-01 -3.65585923e-01]\n",
      " [ 5.01106799e-01 -3.89208347e-01  2.92661905e-01 -9.02752697e-01]\n",
      " [-2.16547251e+00 -1.15172362e+00  9.34515536e-01  4.44703966e-01]\n",
      " [-6.40962553e+00  2.19020646e+09  1.49296427e+00  2.20951176e+01]\n",
      " [-2.79389471e-01 -3.27371985e-01 -1.06303006e-01 -3.38447317e-02]\n",
      " [-3.30046844e+00  1.08071078e+09  8.95173073e-01  2.20951176e+01]]\n"
     ]
    }
   ],
   "source": [
    "eps = np.finfo(height.dtype).eps\n",
    "height = np.maximum(height, eps)\n",
    "width = np.maximum(width, eps)\n",
    "dy = (base_ctr_y - ctr_y) / height\n",
    "dx = (base_ctr_x - ctr_x) / width\n",
    "dh = np.log(base_height / height)\n",
    "dw = np.log(base_width / width)\n",
    "gt_roi_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "print(gt_roi_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have gt_roi_locs and gt_roi_labels for the sampled rois. We now need design the Fast rcnn network and predict the locs and labels, Which we will do in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast R-CNN used ROI pooling to extract features for each and every proposal suggested by selective search (Fast RCNN) or Region Proposal network (RPN in Faster R- CNN). We will see how this ROI pooling works and later pass the rpn proposals which we have computed in section 4 to this layer. Further we will see how this layer is connected to a classification and regression layer to compute the class probabilities and bounding boxes coordinates respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Region of interest pooling (also known as RoI pooling) purpose is to perform max pooling on inputs of non-uniform sizes to obtain fixed-size feature maps (e.g. 7×7). This layer takes two inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fixed-size feature map obtained from a deep convolutional network with several convolutions and max-pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Nx5 matrix of representing a list of regions of interest, where N is the number of RoIs. The first column represents the image index and the remaining four are the co-ordinates of the top left and bottom right corners of the region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the RoI pooling actually do? For every region of interest from the input list, it takes a section of the input feature map that corresponds to it and scales it to some pre-defined size (e.g., 7×7). The scaling is done by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing the region proposal into equal-sized sections (the number of which is the same as the dimension of the output)\n",
    "Finding the largest value in each section\n",
    "Copying these max values to the output buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is that from a list of rectangles with different sizes we can quickly get a list of corresponding feature maps with a fixed size. Note that the dimension of the RoI pooling output doesn’t actually depend on the size of the input feature map nor on the size of the region proposals. It’s determined solely by the number of sections we divide the proposal into. What’s the benefit of RoI pooling? One of them is processing speed. If there are multiple object proposals on the frame (and usually there’ll be a lot of them), we can still use the same input feature map for all of them. Since computing the convolutions at early stages of processing is very expensive, this approach can save us a lot of time. The diagram below shows the working of ROI pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Maxpool.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous sections we got, gt_roi_locs, gt_roi_labels and sample_rois. We will use the sample_rois as the input to the roi_pooling layer. Note that sample_rois has [N, 4] dimension and each row format is yxhw [y, x, h, w]. We need to two changes to this array,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the index of the image [Here we only have one image]\n",
    "changing the format to xywh.\n",
    "Since sample_rois is a numpy array, we will convert into Tensorflow Tensor. create an roi_indices tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 4) (128,)\n"
     ]
    }
   ],
   "source": [
    "# sample_roi format: [y1,x1,y2,x2]\n",
    "# rois format: [x1, y1, x2, y2]\n",
    "rois = tf.convert_to_tensor(sample_roi[:,[1,0,3,2]]*(1/16.0), dtype=tf.float32)\n",
    "# create the index\n",
    "roi_indices = 0 * np.ones((len(rois),), dtype=np.float32)\n",
    "roi_indices = tf.convert_to_tensor(roi_indices, dtype=tf.float32)\n",
    "\n",
    "print(rois.shape, roi_indices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concat rois and roi_indices, so that we get the tensor with shape [N, 5] (index, x, y, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_and_rois = tf.concat([roi_indices[:, np.newaxis], rois], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 5), dtype=float32, numpy=\n",
       "array([[ 0.        , 23.834318  , 34.661243  , 42.49103   , 43.989677  ],\n",
       "       [ 0.        , 37.145706  , 16.82645   , 50.        , 35.477592  ],\n",
       "       [ 0.        , 10.834319  , 29.661245  , 29.491034  , 38.989677  ],\n",
       "       [ 0.        ,  5.1453986 , 29.826372  , 42.455475  , 48.477474  ],\n",
       "       [ 0.        , 12.834319  , 10.661245  , 31.491034  , 19.989676  ],\n",
       "       [ 0.        , 39.142414  , 24.825603  , 50.        , 43.4763    ],\n",
       "       [ 0.        , 14.834319  , 39.661243  , 33.49103   , 48.989677  ],\n",
       "       [ 0.        , 38.83432   , 17.661245  , 50.        , 26.989676  ],\n",
       "       [ 0.        ,  0.83495456, 22.660633  , 19.49223   , 31.989355  ],\n",
       "       [ 0.        , 16.834318  , 33.661243  , 35.49103   , 42.989677  ],\n",
       "       [ 0.        , 19.31699   ,  4.297845  , 45.684048  , 30.711617  ],\n",
       "       [ 0.        ,  7.834483  , 42.661087  , 26.491343  , 50.        ],\n",
       "       [ 0.        , 12.834319  ,  5.6612453 , 31.491034  , 14.989676  ],\n",
       "       [ 0.        , 38.83432   , 24.661245  , 50.        , 33.989677  ],\n",
       "       [ 0.        ,  0.        , 10.295897  , 25.683147  , 36.715054  ],\n",
       "       [ 0.        ,  3.128595  , 45.82204   , 40.442856  , 50.        ],\n",
       "       [ 0.        , 15.317315  , 40.29725   , 41.683773  , 50.        ],\n",
       "       [ 0.        , 17.834318  , 22.661245  , 36.49103   , 31.989676  ],\n",
       "       [ 0.        , 17.834318  ,  5.6612453 , 36.49103   , 14.989676  ],\n",
       "       [ 0.        , 45.83556   , 42.66005   , 50.        , 50.        ],\n",
       "       [ 0.        , 38.83432   , 34.661243  , 50.        , 43.989677  ],\n",
       "       [ 0.        , 38.83432   , 15.661245  , 50.        , 24.989676  ],\n",
       "       [ 0.        , 17.834318  , 19.661245  , 36.49103   , 28.989676  ],\n",
       "       [ 0.        , 10.834319  , 27.661245  , 29.491034  , 36.989677  ],\n",
       "       [ 0.        , 23.834318  , 13.661245  , 42.49103   , 22.989676  ],\n",
       "       [ 0.        ,  6.3135214 , 41.304203  , 32.686985  , 50.        ],\n",
       "       [ 0.        , 23.905115  ,  0.        , 37.094887  , 11.094885  ],\n",
       "       [ 0.        , 42.835114  , 27.660479  , 50.        , 36.989273  ],\n",
       "       [ 0.        , 20.834318  , 37.661243  , 39.49103   , 46.989677  ],\n",
       "       [ 0.        , 12.834319  , 20.661245  , 31.491034  , 29.989676  ],\n",
       "       [ 0.        , 38.83432   ,  5.6612453 , 50.        , 14.989676  ],\n",
       "       [ 0.        ,  0.        , 30.647346  , 15.51817   , 39.982403  ],\n",
       "       [ 0.        , 14.834319  , 37.661243  , 33.49103   , 46.989677  ],\n",
       "       [ 0.        , 24.144285  ,  0.        , 50.        , 18.477037  ],\n",
       "       [ 0.        , 28.834318  , 36.661243  , 47.49103   , 45.989677  ],\n",
       "       [ 0.        , 44.833748  , 22.661798  , 50.        , 31.989965  ],\n",
       "       [ 0.        , 20.834318  , 18.661245  , 39.49103   , 27.989676  ],\n",
       "       [ 0.        , 44.833748  , 26.661798  , 50.        , 35.989967  ],\n",
       "       [ 0.        , 12.834319  , 33.661243  , 31.491034  , 42.989677  ],\n",
       "       [ 0.        , 11.31699   , 28.297844  , 37.684048  , 50.        ],\n",
       "       [ 0.        , 13.166137  ,  0.        , 50.        , 50.        ],\n",
       "       [ 0.        , 33.145397  , 29.826372  , 50.        , 48.477474  ],\n",
       "       [ 0.        , 29.66514   , 43.83028   , 38.991714  , 50.        ],\n",
       "       [ 0.        , 36.145462  ,  2.8263886 , 50.        , 21.477499  ],\n",
       "       [ 0.        ,  7.31699   , 11.297845  , 33.684048  , 37.711617  ],\n",
       "       [ 0.        ,  0.        , 11.647347  , 15.51817   , 20.9824    ],\n",
       "       [ 0.        ,  5.8343186 , 35.661243  , 24.491034  , 44.989677  ],\n",
       "       [ 0.        , 10.834319  , 31.661245  , 29.491034  , 40.989677  ],\n",
       "       [ 0.        , 29.834318  ,  7.6612453 , 48.49103   , 16.989676  ],\n",
       "       [ 0.        , 14.31699   , 21.297844  , 40.684048  , 47.711617  ],\n",
       "       [ 0.        , 13.145398  , 16.826372  , 50.        , 35.477474  ],\n",
       "       [ 0.        , 27.834318  ,  6.6612453 , 46.49103   , 15.989676  ],\n",
       "       [ 0.        , 22.834318  , 38.661243  , 41.49103   , 47.989677  ],\n",
       "       [ 0.        ,  0.        , 20.253468  , 21.663647  , 46.789764  ],\n",
       "       [ 0.        ,  5.834615  ,  1.6609598 , 24.49159   , 10.989527  ],\n",
       "       [ 0.        , 39.16678   , 16.86499   , 50.        , 50.        ],\n",
       "       [ 0.        ,  8.834319  , 34.661243  , 27.491034  , 43.989677  ],\n",
       "       [ 0.        , 42.16729   , 20.86858   , 50.        , 50.        ],\n",
       "       [ 0.        , 18.834318  , 34.661243  , 37.49103   , 43.989677  ],\n",
       "       [ 0.        , 18.31699   , 20.297844  , 44.684048  , 46.711617  ],\n",
       "       [ 0.        ,  8.324479  , 44.284092  , 34.677708  , 50.        ],\n",
       "       [ 0.        , 36.83432   , 27.661245  , 50.        , 36.989677  ],\n",
       "       [ 0.        , 29.14527   ,  1.826338  , 50.        , 20.477423  ],\n",
       "       [ 0.        , 38.316967  , 15.297887  , 50.        , 41.711544  ],\n",
       "       [ 0.        ,  3.1605573 , 43.83028   , 40.466858  , 50.        ],\n",
       "       [ 0.        , 17.834318  ,  8.661245  , 36.49103   , 17.989676  ],\n",
       "       [ 0.        , 14.31699   , 32.297844  , 40.684048  , 50.        ],\n",
       "       [ 0.        ,  9.31699   , 23.297844  , 35.684048  , 49.711617  ],\n",
       "       [ 0.        , 20.832245  , 44.663242  , 39.487133  , 50.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , 15.52034   ,  8.98182   ],\n",
       "       [ 0.        , 24.834318  , 22.661245  , 43.49103   , 31.989676  ],\n",
       "       [ 0.        , 22.834318  ,  6.6612453 , 41.49103   , 15.989676  ],\n",
       "       [ 0.        , 17.83441   ,  2.661158  , 36.491203  , 11.989631  ],\n",
       "       [ 0.        ,  7.8387985 , 47.65693   , 26.49946   , 50.        ],\n",
       "       [ 0.        , 10.834319  , 35.661243  , 29.491034  , 44.989677  ],\n",
       "       [ 0.        , 43.835354  ,  0.        , 50.        ,  8.989154  ],\n",
       "       [ 0.        , 26.31699   ,  8.297845  , 50.        , 34.711617  ],\n",
       "       [ 0.        ,  3.8343186 , 36.661243  , 22.491034  , 45.989677  ],\n",
       "       [ 0.        , 12.31699   , 12.297845  , 38.684048  , 38.711617  ],\n",
       "       [ 0.        ,  7.905115  , 45.905113  , 21.094885  , 50.        ],\n",
       "       [ 0.        , 44.32023   ,  0.        , 50.        , 22.722092  ],\n",
       "       [ 0.        ,  7.8343186 , 15.661245  , 26.491034  , 24.989676  ],\n",
       "       [ 0.        ,  0.        , 31.253468  , 21.663647  , 50.        ],\n",
       "       [ 0.        , 35.128593  , 45.82204   , 50.        , 50.        ],\n",
       "       [ 0.        , 37.145706  , 12.826451  , 50.        , 31.477594  ],\n",
       "       [ 0.        ,  0.        , 23.647346  , 15.51817   , 32.982403  ],\n",
       "       [ 0.        ,  0.        ,  0.        , 30.483011  , 50.        ],\n",
       "       [ 0.        ,  6.8343186 ,  7.6612453 , 25.491034  , 16.989676  ],\n",
       "       [ 0.        , 45.83551   , 19.660097  , 50.        , 28.989075  ],\n",
       "       [ 0.        , 26.31699   , 16.297844  , 50.        , 42.711617  ],\n",
       "       [ 0.        , 25.834318  , 10.661245  , 44.49103   , 19.989676  ],\n",
       "       [ 0.        , 38.316967  , 10.297887  , 50.        , 36.711544  ],\n",
       "       [ 0.        ,  9.834319  , 37.661243  , 28.491034  , 46.989677  ],\n",
       "       [ 0.        , 38.83432   ,  7.6612453 , 50.        , 16.989676  ],\n",
       "       [ 0.        , 22.834318  , 40.661243  , 41.49103   , 49.989677  ],\n",
       "       [ 0.        ,  1.8344549 ,  3.6611142 , 20.49129   , 12.989608  ],\n",
       "       [ 0.        , 22.830278  ,  0.        , 41.48343   , 30.46686   ],\n",
       "       [ 0.        , 39.14148   , 41.825363  , 50.        , 50.        ],\n",
       "       [ 0.        , 44.31898   , 23.29419   , 50.        , 49.718063  ],\n",
       "       [ 0.        , 42.14968   , 43.827477  , 50.        , 50.        ],\n",
       "       [ 0.        , 31.838799  , 47.65693   , 50.        , 50.        ],\n",
       "       [ 0.        , 44.833748  , 11.661798  , 50.        , 20.989965  ],\n",
       "       [ 0.        ,  0.        , 46.66443   , 18.48482   , 50.        ],\n",
       "       [ 0.        ,  9.665139  , 44.83028   , 18.991714  , 50.        ],\n",
       "       [ 0.        , 42.168247  ,  0.        , 50.        , 50.        ],\n",
       "       [ 0.        , 16.834318  , 31.661245  , 35.49103   , 40.989677  ],\n",
       "       [ 0.        ,  3.8343186 , 38.661243  , 22.491034  , 47.989677  ],\n",
       "       [ 0.        , 12.83333   ,  0.        , 31.489174  ,  8.990175  ],\n",
       "       [ 0.        , 42.835114  , 35.660477  , 50.        , 44.989273  ],\n",
       "       [ 0.        , 42.835506  ,  1.6601021 , 50.        , 10.989078  ],\n",
       "       [ 0.        ,  4.8343186 , 40.661243  , 23.491034  , 49.989677  ],\n",
       "       [ 0.        , 45.835514  ,  4.660096  , 50.        , 13.989075  ],\n",
       "       [ 0.        , 39.142414  , 20.825603  , 50.        , 39.4763    ],\n",
       "       [ 0.        , 11.165756  , 17.857773  , 48.470142  , 50.        ],\n",
       "       [ 0.        ,  0.        , 13.81243   , 30.414845  , 32.45622   ],\n",
       "       [ 0.        ,  0.        ,  1.6459829 , 15.520835  , 10.981688  ],\n",
       "       [ 0.        , 22.834318  , 36.661243  , 41.49103   , 45.989677  ],\n",
       "       [ 0.        , 13.324479  , 44.284092  , 39.677708  , 50.        ],\n",
       "       [ 0.        , 32.83432   , 13.661245  , 50.        , 22.989676  ],\n",
       "       [ 0.        , 25.834318  , 26.661245  , 44.49103   , 35.989677  ],\n",
       "       [ 0.        , 34.83432   , 38.661243  , 50.        , 47.989677  ],\n",
       "       [ 0.        , 34.834354  ,  3.661212  , 50.        , 12.989659  ],\n",
       "       [ 0.        , 45.83551   ,  7.6600976 , 50.        , 16.989075  ],\n",
       "       [ 0.        , 27.834318  , 15.661245  , 46.49103   , 24.989676  ],\n",
       "       [ 0.        , 28.834318  , 28.661245  , 47.49103   , 37.989677  ],\n",
       "       [ 0.        ,  8.832246  , 44.663242  , 27.487131  , 50.        ],\n",
       "       [ 0.        , 11.31699   ,  7.297845  , 37.684048  , 33.711617  ],\n",
       "       [ 0.        , 10.317315  , 40.29725   , 36.683773  , 50.        ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_and_rois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to pass this array to the roi_pooling layer. We will briefly discuss the workings of it here. The sudo code is as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multiply the dimensions of rois with the sub_sampling ratio (16 in this case)\n",
    "- Empty output Tensor\n",
    "- Take each roi\n",
    "    - subset the feature map based on the roi dimension\n",
    "    - Apply AdaptiveMaxPool2d to this subset Tensor.\n",
    "    - Add the outputs to the output Tensor\n",
    "- Empty output Tensor goes to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 4), dtype=float32, numpy=\n",
       "array([[381.3491  , 554.5799  , 703.83484 , 679.8565  ],\n",
       "       [594.3313  , 269.2232  , 567.6415  , 800.      ],\n",
       "       [173.3491  , 474.57993 , 623.83484 , 471.85654 ],\n",
       "       [ 82.32638 , 477.22195 , 775.6396  , 679.2876  ],\n",
       "       [205.3491  , 170.57993 , 319.8348  , 503.85654 ],\n",
       "       [626.2786  , 397.20966 , 695.6208  , 800.      ],\n",
       "       [237.3491  , 634.5799  , 783.83484 , 535.8565  ],\n",
       "       [621.3491  , 282.57993 , 431.8348  , 800.      ],\n",
       "       [ 13.359273, 362.57013 , 511.82968 , 311.87567 ],\n",
       "       [269.3491  , 538.5799  , 687.83484 , 567.8565  ],\n",
       "       [309.07184 ,  68.76552 , 491.38586 , 730.94476 ],\n",
       "       [125.35173 , 682.5774  , 800.      , 423.86148 ],\n",
       "       [205.3491  ,  90.579926, 239.83482 , 503.85654 ],\n",
       "       [621.3491  , 394.57993 , 543.83484 , 800.      ],\n",
       "       [  0.      , 164.73434 , 587.44086 , 410.93036 ],\n",
       "       [ 50.05752 , 733.15265 , 800.      , 647.0857  ],\n",
       "       [245.07704 , 644.756   , 800.      , 666.94037 ],\n",
       "       [285.3491  , 362.57993 , 511.8348  , 583.8565  ],\n",
       "       [285.3491  ,  90.579926, 239.83482 , 583.8565  ],\n",
       "       [733.36896 , 682.5608  , 800.      , 800.      ],\n",
       "       [621.3491  , 554.5799  , 703.83484 , 800.      ],\n",
       "       [621.3491  , 250.57993 , 399.8348  , 800.      ],\n",
       "       [285.3491  , 314.57993 , 463.8348  , 583.8565  ],\n",
       "       [173.3491  , 442.57993 , 591.83484 , 471.85654 ],\n",
       "       [381.3491  , 218.57993 , 367.8348  , 679.8565  ],\n",
       "       [101.01634 , 660.86725 , 800.      , 522.99176 ],\n",
       "       [382.48184 ,   0.      , 177.51816 , 593.5182  ],\n",
       "       [685.3618  , 442.56766 , 591.82837 , 800.      ],\n",
       "       [333.3491  , 602.5799  , 751.83484 , 631.8565  ],\n",
       "       [205.3491  , 330.57993 , 479.8348  , 503.85654 ],\n",
       "       [621.3491  ,  90.579926, 239.83482 , 800.      ],\n",
       "       [  0.      , 490.35754 , 639.71844 , 248.29073 ],\n",
       "       [237.3491  , 602.5799  , 751.83484 , 535.8565  ],\n",
       "       [386.30856 ,   0.      , 295.6326  , 800.      ],\n",
       "       [461.3491  , 586.5799  , 735.83484 , 759.8565  ],\n",
       "       [717.33997 , 362.58878 , 511.83945 , 800.      ],\n",
       "       [333.3491  , 298.57993 , 447.8348  , 631.8565  ],\n",
       "       [717.33997 , 426.58878 , 575.8395  , 800.      ],\n",
       "       [205.3491  , 538.5799  , 687.83484 , 503.85654 ],\n",
       "       [181.07184 , 452.7655  , 800.      , 602.94476 ],\n",
       "       [210.65819 ,   0.      , 800.      , 800.      ],\n",
       "       [530.32635 , 477.22195 , 775.6396  , 800.      ],\n",
       "       [474.64224 , 701.2845  , 800.      , 623.86743 ],\n",
       "       [578.3274  ,  45.222218, 343.63998 , 800.      ],\n",
       "       [117.07184 , 180.76552 , 603.38586 , 538.94476 ],\n",
       "       [  0.      , 186.35756 , 335.7184  , 248.29073 ],\n",
       "       [ 93.3491  , 570.5799  , 719.83484 , 391.85654 ],\n",
       "       [173.3491  , 506.57993 , 655.83484 , 471.85654 ],\n",
       "       [477.3491  , 122.579926, 271.8348  , 775.8565  ],\n",
       "       [229.07184 , 340.7655  , 763.38586 , 650.94476 ],\n",
       "       [210.32637 , 269.22195 , 567.6396  , 800.      ],\n",
       "       [445.3491  , 106.579926, 255.83482 , 743.8565  ],\n",
       "       [365.3491  , 618.5799  , 767.83484 , 663.8565  ],\n",
       "       [  0.      , 324.05548 , 748.6362  , 346.61835 ],\n",
       "       [ 93.35384 ,  26.575357, 175.83243 , 391.86545 ],\n",
       "       [626.66846 , 269.83984 , 800.      , 800.      ],\n",
       "       [141.3491  , 554.5799  , 703.83484 , 439.85654 ],\n",
       "       [674.67664 , 333.89728 , 800.      , 800.      ],\n",
       "       [301.3491  , 554.5799  , 703.83484 , 599.8565  ],\n",
       "       [293.07184 , 324.7655  , 747.38586 , 714.94476 ],\n",
       "       [133.19167 , 708.5455  , 800.      , 554.8433  ],\n",
       "       [589.3491  , 442.57993 , 591.83484 , 800.      ],\n",
       "       [466.3243  ,  29.221409, 327.63876 , 800.      ],\n",
       "       [613.0715  , 244.76619 , 667.3847  , 800.      ],\n",
       "       [ 50.568916, 701.2845  , 800.      , 647.4697  ],\n",
       "       [285.3491  , 138.57993 , 287.8348  , 583.8565  ],\n",
       "       [229.07184 , 516.7655  , 800.      , 650.94476 ],\n",
       "       [149.07184 , 372.7655  , 795.38586 , 570.94476 ],\n",
       "       [333.31592 , 714.6119  , 800.      , 631.7941  ],\n",
       "       [  0.      ,   0.      , 143.70912 , 248.32544 ],\n",
       "       [397.3491  , 362.57993 , 511.8348  , 695.8565  ],\n",
       "       [365.3491  , 106.579926, 255.83482 , 663.8565  ],\n",
       "       [285.35056 ,  42.57853 , 191.83409 , 583.85925 ],\n",
       "       [125.42078 , 762.51086 , 800.      , 423.99136 ],\n",
       "       [173.3491  , 570.5799  , 719.83484 , 471.85654 ],\n",
       "       [701.36566 ,   0.      , 143.82646 , 800.      ],\n",
       "       [421.07184 , 132.76552 , 555.38586 , 800.      ],\n",
       "       [ 61.3491  , 586.5799  , 735.83484 , 359.85654 ],\n",
       "       [197.07184 , 196.76552 , 619.38586 , 618.94476 ],\n",
       "       [126.48184 , 734.4818  , 800.      , 337.51816 ],\n",
       "       [709.12366 ,   0.      , 363.55347 , 800.      ],\n",
       "       [125.3491  , 250.57993 , 399.8348  , 423.85654 ],\n",
       "       [  0.      , 500.05548 , 800.      , 346.61835 ],\n",
       "       [562.0575  , 733.15265 , 800.      , 800.      ],\n",
       "       [594.3313  , 205.22322 , 503.6415  , 800.      ],\n",
       "       [  0.      , 378.35754 , 527.71844 , 248.29073 ],\n",
       "       [  0.      ,   0.      , 800.      , 487.72818 ],\n",
       "       [109.3491  , 122.579926, 271.8348  , 407.85654 ],\n",
       "       [733.36816 , 314.56155 , 463.8252  , 800.      ],\n",
       "       [421.07184 , 260.7655  , 683.38586 , 800.      ],\n",
       "       [413.3491  , 170.57993 , 319.8348  , 711.8565  ],\n",
       "       [613.0715  , 164.76619 , 587.3847  , 800.      ],\n",
       "       [157.3491  , 602.5799  , 751.83484 , 455.85654 ],\n",
       "       [621.3491  , 122.579926, 271.8348  , 800.      ],\n",
       "       [365.3491  , 650.5799  , 799.83484 , 663.8565  ],\n",
       "       [ 29.351278,  58.577827, 207.83372 , 327.86063 ],\n",
       "       [365.28445 ,   0.      , 487.46976 , 663.73486 ],\n",
       "       [626.2637  , 669.2058  , 800.      , 800.      ],\n",
       "       [709.1037  , 372.70703 , 795.489   , 800.      ],\n",
       "       [674.3949  , 701.2396  , 800.      , 800.      ],\n",
       "       [509.42078 , 762.51086 , 800.      , 800.      ],\n",
       "       [717.33997 , 186.58876 , 335.83945 , 800.      ],\n",
       "       [  0.      , 746.63086 , 800.      , 295.7571  ],\n",
       "       [154.64223 , 717.2845  , 800.      , 303.86743 ],\n",
       "       [674.69196 ,   0.      , 800.      , 800.      ],\n",
       "       [269.3491  , 506.57993 , 655.83484 , 567.8565  ],\n",
       "       [ 61.3491  , 618.5799  , 767.83484 , 359.85654 ],\n",
       "       [205.33328 ,   0.      , 143.8428  , 503.82678 ],\n",
       "       [685.3618  , 570.5676  , 719.82837 , 800.      ],\n",
       "       [685.3681  ,  26.561634, 175.82524 , 800.      ],\n",
       "       [ 77.3491  , 650.5799  , 799.83484 , 375.85654 ],\n",
       "       [733.3682  ,  74.56154 , 223.8252  , 800.      ],\n",
       "       [626.2786  , 333.20966 , 631.6208  , 800.      ],\n",
       "       [178.6521  , 285.72437 , 800.      , 775.5223  ],\n",
       "       [  0.      , 220.99889 , 519.2995  , 486.6375  ],\n",
       "       [  0.      ,  26.335726, 175.707   , 248.33336 ],\n",
       "       [365.3491  , 586.5799  , 735.83484 , 663.8565  ],\n",
       "       [213.19167 , 708.5455  , 800.      , 634.8433  ],\n",
       "       [525.3491  , 218.57993 , 367.8348  , 800.      ],\n",
       "       [413.3491  , 426.57993 , 575.83484 , 711.8565  ],\n",
       "       [557.3491  , 618.5799  , 767.83484 , 800.      ],\n",
       "       [557.3497  ,  58.57939 , 207.83455 , 800.      ],\n",
       "       [733.36816 , 122.56156 , 271.8252  , 800.      ],\n",
       "       [445.3491  , 250.57993 , 399.8348  , 743.8565  ],\n",
       "       [461.3491  , 458.57993 , 607.83484 , 759.8565  ],\n",
       "       [141.31593 , 714.6119  , 800.      , 439.7941  ],\n",
       "       [181.07184 , 116.76552 , 539.38586 , 602.94476 ],\n",
       "       [165.07704 , 644.756   , 800.      , 586.94037 ]], dtype=float32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_and_rois[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (7, 7)\n",
    "\n",
    "# rois format: [index, x1, y1, x2, y2]\n",
    "# adaptive_max_pool = AdaptiveMaxPool2d(size[0], size[1])\n",
    "adptive_max_pool = tfa.layers.AdaptiveMaxPooling2D(output_size=size, format='channel_last')\n",
    "\n",
    "output = []\n",
    "\n",
    "# rois = indices_and_rois.data.float()\n",
    "rois = indices_and_rois\n",
    "\n",
    "# rois[:, 1:].mul_(1/16.0) # Subsampling ratio\n",
    "# mul Multiplies each element of the input input with the scalar other and returns a new resulting tensor.\n",
    "# mul_ the \"_\" stands for inplace\n",
    "#rois[:, 1:] = tf.math.scalar_mul(1/16.0, rois[:, 1:])\n",
    "\n",
    "\n",
    "# rois = rois.long(), convert to signed int64\n",
    "rois = tf.dtypes.cast(rois, tf.int32)\n",
    "\n",
    "#num_rois = rois.size(0)\n",
    "num_rois = rois.shape[0]\n",
    "\n",
    "for i in range(num_rois):\n",
    "    # get a roi\n",
    "    roi = rois[i]\n",
    "    # get the index image\n",
    "    im_idx = roi[0]\n",
    "    # rois format: [index, x1, y1, x2, y2]\n",
    "    # im = out_map.narrow(0, im_idx, 1)[..., roi[2]:(roi[4]+1), roi[1]:(roi[3]+1)]\n",
    "    # im = out_map.narrow(0, im_idx, 1)[..., y1:y2+1, x1:x2+1]\n",
    "    im = out_map[:,roi[2]:(roi[4]+1),roi[1]:(roi[3]+1),:]\n",
    "    with tf.device('CPU:0'):\n",
    "        x = adptive_max_pool(im)\n",
    "    output.append(x)\n",
    "    \n",
    "output = tf.concat(output, 0)\n",
    "print(output.shape)\n",
    "\n",
    "#reshape the tensor\n",
    "k = tf.reshape(output,[output.shape[0] ,-1])\n",
    "print(k.shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[[[1,2],[3,4]],[[5,6],[6,7]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 2, 2), dtype=int32, numpy=\n",
       "array([[[[1, 2],\n",
       "         [3, 4]],\n",
       "\n",
       "        [[5, 6],\n",
       "         [6, 7]]]])>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 1, 1), dtype=int32, numpy=\n",
       "array([[[[4]],\n",
       "\n",
       "        [[7]]]])>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[...,1:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_and_rois.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = indices_and_rois[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= tf.math.scalar_mul(1/16.0, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.dtypes.cast(a, tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 0, 23, 34, 43, 42])>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_addons.layers' has no attribute 'AdaptiveMaxPooling2D'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-bdaa5a25a2f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0madptive_max_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdaptiveMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'channel_last'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_addons.layers' has no attribute 'AdaptiveMaxPooling2D'"
     ]
    }
   ],
   "source": [
    "adptive_max_pool = tfa.layers.AdaptiveMaxPooling2D(output_size=size, data_format='channel_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.1'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfa.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
