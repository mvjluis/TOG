{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASTER R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction\n",
    "# create random image\n",
    "image = np.zeros((1,3,800,800))\n",
    "#change type to float\n",
    "image = image.astype(np.float32)\n",
    "# convert to tensor\n",
    "image = tf.convert_to_tensor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using just tensorflow to create generic data of ...\n",
    "# image\n",
    "image = tf.zeros([1,800,800,3], tf.float32)\n",
    "# bbox\n",
    "bbox = tf.constant([[20, 30, 400, 500], [300, 400, 500, 600]])\n",
    "# labels for each bbox\n",
    "labels = tf.constant([6,8]) \n",
    "sub_sample = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VGG16 network is used as a feature extraction module here, This acts as a backbone for both the RPN network and Fast_R-CNN network. We need to make a few changes to the VGG network inorder to make this work. Since the input of the network is 800, the output of the feature extraction module should have a feature map size of (800//16). So we need to check where the VGG16 module is achieving this feature map size and trim the network till der. This can be done in the following way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch implementation:\n",
    "https://medium.com/@fractaldle/guide-to-build-faster-rcnn-in-pytorch-95b10c273439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16  import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 11s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# The default input size for this model is 224x224.\n",
    "vgg = VGG16(weights='imagenet', include_top=False, input_tensor=layers.Input(shape=[800, 800, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x1ee5787d388>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1ee5787dd08>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1ee57848dc8>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1ee579ea1c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1ee579df608>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1ee579f3148>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1ee579e6ac8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1ee57a06408>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1ee57a15d48>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1ee57a17a48>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1ee57a21248>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1ee57a2b388>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1ee57a34c88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1ee57a37988>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1ee57a409c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1ee57a4a308>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1ee57a56c48>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1ee57a58948>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1ee57a60988>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the layers\n",
    "vgg.layers\n",
    "# or uncomment the next line\n",
    "#vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vgg feature layers:  19\n",
      "brake\n",
      "shape of ouput:  (1, 50, 50, 512)\n",
      "number of layers needed:  18\n"
     ]
    }
   ],
   "source": [
    "#how to pass a input test tensor through the layers without compile the model?\n",
    "im = image\n",
    "fe_layers = vgg.layers\n",
    "l_c = 0\n",
    "req_layers = []\n",
    "print(\"size of vgg feature layers: \", len(fe_layers))\n",
    "# Use CPU to test the convolutions\n",
    "with tf.device('CPU:0'):\n",
    "    for i in fe_layers:\n",
    "        im = i(im)\n",
    "        if im.shape[1] < 800//16:\n",
    "            print('brake')\n",
    "            break\n",
    "        req_layers.append(i)\n",
    "        l_c += 1\n",
    "        out_channels = im.shape\n",
    "print('shape of ouput: ', out_channels)\n",
    "print('number of layers needed: ', l_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x000001EE5787D388>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001EE5787DD08>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001EE57848DC8>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001EE579EA1C8>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001EE579DF608>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001EE579F3148>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001EE579E6AC8>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001EE57A06408>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001EE57A15D48>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001EE57A17A48>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001EE57A21248>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001EE57A2B388>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001EE57A34C88>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001EE57A37988>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001EE57A409C8>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001EE57A4A308>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001EE57A56C48>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001EE57A58948>]\n"
     ]
    }
   ],
   "source": [
    "# print the required layers\n",
    "print(req_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the bakcbone for fast r-cnn\n",
    "input_fe = layers.Input(shape=[800, 800, 3])\n",
    "fe_extractor = req_layers[1](input_fe)\n",
    "for l in range(2,len(req_layers)):\n",
    "    fe_extractor = req_layers[l](fe_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of output:  (1, 50, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "# Test feature extractor model output\n",
    "fe_model = Model(inputs=input_fe, outputs=fe_extractor)\n",
    "im = image\n",
    "with tf.device('CPU:0'):\n",
    "    im = fe_model(im)\n",
    "print(\"shape of output: \",im.shape)\n",
    "del(fe_model)\n",
    "\n",
    "# Input = input_fe,\n",
    "# Output of extractor feature = fe_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use anchor_scales of 8, 16, 32, ratio of 0.5, 1, 2 and sub sampling of 16 (Since we have pooled our image from 800 px to 50px). Now every pixel in the output feature map maps to corresponding 16 * 16 pixels in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each pixel location on the feature map, We need to generate 9 anchor boxes (number of anchor_scales and number of ratios) and each anchor box will have ‘y1’, ‘x1’, ‘y2’, ‘x2’. So at each location anchor will have a shape of (9, 4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# As an example will be created 9 anchor boxes space for 'y1', 'x1', 'y2', 'x2'\n",
    "ratios = [0.5, 1, 2]\n",
    "anchor_scales = [8, 16, 32]\n",
    "anchor_base = np.zeros((len(ratios)*len(anchor_scales), 4), dtype=np.float32)\n",
    "\n",
    "print(anchor_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center: yc = 8.0, xc = 8.0\n"
     ]
    }
   ],
   "source": [
    "# fill the anchor axis values for the 9 anchor boxes of the respective size and ratio\n",
    "sub_sample = 16\n",
    "ctr_y = sub_sample / 2.\n",
    "ctr_x = sub_sample / 2.\n",
    "\n",
    "print('center of the sample anchor boxes: yc = {}, xc = {}'.format(ctr_y, ctr_x))\n",
    "for i in range(len(ratios)):\n",
    "    for j in range(len(anchor_scales)):\n",
    "        h = sub_sample * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
